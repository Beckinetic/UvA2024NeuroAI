{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSi9i1FM8b2P"
      },
      "source": [
        "# Neuro-AI: Representational Similarity Analysis\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dVgdnsig_ce"
      },
      "source": [
        "# Tutorial Day 2 - Representational Similarrity Analysis\n",
        "\n",
        "<!-- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1wOwyPqY3X-pSIiRHk3wsv1zAhGtkHbRO?usp=sharing)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1-WDElsPrfl-UJqwB6xM7HFYVkBiP2qiC) -->\n",
        "\n",
        "\n",
        "> By Giacomo Aldegheri, Wieger Scheurer and Julio Smidi\n",
        "\n",
        "\n",
        "In the first part of this tutorial, we will look at *representational similarity analysis* (RSA, [original paper](https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full)). RSA is a commonly used way to explore and analyse how different stimuli are represented. A big advantage of RSA is that it can compare these representations across different modalities of measurement (for example fMRI, EEG or behavioral responses) and even across subjects and species (humans, monkeys and also neural networks!).\n",
        "\n",
        "For RSA we create a *representational (dis)similarity matrix* (RDM) for each entity, which are made by taking a (dis)similarity measure for the activity patterns between each stimulus pair. One often used measure for this is the correlation distance. With such an RDM, we can now directly compare this to any other RDM, to explore the representational structure differences!\n",
        "\n",
        "In this tutorial, we will again use fMRI data from the 2023 Algonauts challenge. It is a subset of the Natural Scenes Dataset (NSD) only including the visual system. The full dataset includes ~30,000 images per subject, but here, to make computations quicker, we will only use 872 images that were seen by all subjects.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/WiegerScheurer/rsa_neuroai/main/rsa_overview.jpeg\" alt=\"General RSA overview\" width=\"1000\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This tutorial is heavily indebted to the following resources:\n",
        "\n",
        "- [Algonauts 2023 Challenge tutorial](https://colab.research.google.com/drive/1bLJGP3bAo_hAOwZPHpiSHKlt97X9xsUw?usp=share_link#scrollTo=gjQrI9AlzDqG)\n",
        "- [Deep NSD tutorial by Colin Conwell](https://colab.research.google.com/drive/1OalDuiQ6Dwg39XT-BkPMz2XAduPUxQtv?usp=sharing)\n",
        "- Relating DCNN features with brain activity tutorial by Jessica Loke\n",
        "- [Comparing brains and DNNs presentation by Martin Hebart](http://algonauts.csail.mit.edu/slides/Martin_Hebart-Comparing_Brains_and_DNNs.pdf)\n",
        "- [RSA toolbox by the RSAgroup](https://rsatoolbox.readthedocs.io/en/latest/overview.html)\n",
        "- [Ni-edu by Lukas Snoek](https://lukas-snoek.com/NI-edu/fMRI-pattern-analysis/week_3/rsa.html#categorical-rdms)\n",
        "- [Voxelwise encoding tutorials by Matteo Visconti di Oleggio Castello (Gallant lab)](https://github.com/gallantlab/voxelwise_tutorials)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vKV5xtL8rG_"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhybE3pajXf3"
      },
      "source": [
        "## Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnxvl71zARIu"
      },
      "outputs": [],
      "source": [
        "!pip install nilearn==0.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ElcHg-C8jyV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageOps\n",
        "from collections import OrderedDict\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from nilearn import datasets, plotting\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import MDS\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "from scipy.stats import pearsonr, spearmanr, rankdata\n",
        "from scipy.spatial.distance import cosine\n",
        "import random\n",
        "import glob\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P60jEu26ybpq"
      },
      "outputs": [],
      "source": [
        "# Optional: uncomment if you want to use OpenAI's CLIP model:\n",
        "#!pip install git+https://github.com/openai/CLIP.git\n",
        "#import clip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg_v8BLTqRcn"
      },
      "source": [
        "## Quick notebook execution\n",
        "It is possible that you will have to restart the notebook and execute it from scratch (e.g. sometimes the Runtime gets disconnected). Toggle this to skip execution of computationally intensive cells (e.g. 3D plots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OA5gFo46qQwh"
      },
      "outputs": [],
      "source": [
        "quick_execution = True # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlO9eCHpF3yb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bGj5NI3ILB9"
      },
      "source": [
        "## Set device\n",
        "If you have the possibility, using the GPU (CUDA) runtime will make computations significantly faster. In Colab, you can press the arrow in the top-right next to the RAM and Disk icons, select \"Change runtime type\" and select the T4 GPU. In the free version of Colab the usage of this is limited however."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UUM-Y0a2-K1E"
      },
      "outputs": [],
      "source": [
        "device = 'cpu' #@param ['cpu', 'cuda'] {allow-input: true}\n",
        "if device=='cuda':\n",
        "    assert torch.cuda.is_available()\n",
        "device = torch.device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfxTP84Z9PGR"
      },
      "source": [
        "## Get data\n",
        "\n",
        "First, you need to access the data from [this public folder](https://drive.google.com/drive/folders/1AjDOejWLjfXGkr-hK07SZJ_4ni1nypjw?usp=sharing). Before running this tutorial, you need to select the folder and choose \"Add a shortcut to Drive\". This will create a shortcut (without copying or taking space) of the folder to a desired path in your Google Drive, from which you will be able to access the content.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=15TNjV__sWCcnBRlxbXNbJfpidx-C6nrk' width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iP3tkB6C86TN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "main_dir = '/content/drive/MyDrive/UvA_encodingtutorial' #@param {type:\"string\"}\n",
        "fmri_dir = os.path.join(main_dir, 'fmri_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1Jsiqmy9jpH"
      },
      "outputs": [],
      "source": [
        "assert os.path.isdir(main_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7llmuw7_92fo"
      },
      "outputs": [],
      "source": [
        "# The folders of all eight subjects should be there\n",
        "os.listdir(fmri_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDA4lf00ixnJ"
      },
      "source": [
        "# Load and visualize the data\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/WiegerScheurer/rsa_neuroai/main/breinvlam4.png\" width=\"250\"/>\n",
        "\n",
        "The fMRI data consists of two ```.npy``` files:\n",
        "- ```lh_training_fmri.npy```: the left hemisphere (LH) fMRI data.\n",
        "- ```rh_training_fmri.npy```: the right hemisphere (RH) fMRI data.\n",
        "\n",
        "Both files are 2D arrays, where each row is a stimulus image and each column is an fMRI vertex.\n",
        "\n",
        "\n",
        "> For more information on the fMRI responses please check the [README.txt](https://www.google.com/url?q=https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F16oLCaDmUBZuT6z_VGKO-qzwidYDE77Sg%2Fview%3Fusp%3Dshare_link) file from the 2023 Algonauts Challenge.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0B1QdJxVA9b"
      },
      "outputs": [],
      "source": [
        "# @title Utilities to plot ROIs and activations\n",
        "\n",
        "def get_roi_mask(roi, hemisphere, subj):\n",
        "\n",
        "  subj_dir = os.path.join(fmri_dir, f'subj{subj:02d}')\n",
        "\n",
        "  # Define the ROI class based on the selected ROI\n",
        "  if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
        "      roi_class = 'prf-visualrois'\n",
        "  elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
        "      roi_class = 'floc-bodies'\n",
        "  elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
        "      roi_class = 'floc-faces'\n",
        "  elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
        "      roi_class = 'floc-places'\n",
        "  elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
        "      roi_class = 'floc-words'\n",
        "  elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
        "      roi_class = 'streams'\n",
        "  elif roi == 'all-vertices':\n",
        "      roi_class = roi\n",
        "\n",
        "  # Load the ROI brain surface maps\n",
        "  fsaverage_roi_class_dir = os.path.join(subj_dir, 'roi_masks',\n",
        "      hemisphere[0]+'h.'+roi_class+'_fsaverage_space.npy')\n",
        "\n",
        "  fsaverage_roi_class = np.load(fsaverage_roi_class_dir)\n",
        "\n",
        "  if roi != 'all-vertices':\n",
        "    challenge_roi_class_dir = os.path.join(subj_dir, 'roi_masks',\n",
        "      hemisphere[0]+'h.'+roi_class+'_challenge_space.npy')\n",
        "\n",
        "    challenge_roi_class = np.load(challenge_roi_class_dir)\n",
        "\n",
        "    roi_map_dir = os.path.join(subj_dir, 'roi_masks',\n",
        "        'mapping_'+roi_class+'.npy')\n",
        "    roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n",
        "\n",
        "    # Select the vertices corresponding to the ROI of interest\n",
        "    roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
        "    challenge_roi = np.asarray(challenge_roi_class == roi_mapping, dtype=int)\n",
        "    fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n",
        "\n",
        "    return challenge_roi, fsaverage_roi\n",
        "\n",
        "  else:\n",
        "    return None, fsaverage_roi_class\n",
        "\n",
        "\n",
        "def plot_surf_map(subj=1, stat_map=None, hemi='left', roi=None, title=None):\n",
        "  \"\"\"\n",
        "  Plot ROI or statistical\n",
        "  \"\"\"\n",
        "  if roi in [None, 'all-vertices']:\n",
        "    roi = 'all-vertices'\n",
        "\n",
        "  challenge_roi, fsaverage_roi = get_roi_mask(roi, hemi, subj)\n",
        "\n",
        "  # Map the fMRI data onto the brain surface map\n",
        "\n",
        "\n",
        "  if stat_map is None:\n",
        "    fsaverage_response = fsaverage_roi\n",
        "    cmap = 'cool'\n",
        "    colorbar = False\n",
        "  else:\n",
        "    fsaverage_response = np.zeros(len(fsaverage_roi))\n",
        "    if roi != 'all-vertices':\n",
        "      fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
        "        stat_map[np.where(challenge_roi)[0]]\n",
        "    else:\n",
        "      fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
        "        stat_map\n",
        "    cmap = 'cold_hot'\n",
        "    colorbar = True\n",
        "\n",
        "  if title is None:\n",
        "    title = roi+', '+hemi+' hemisphere'\n",
        "\n",
        "  # Create the interactive brain surface map\n",
        "  fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
        "  view = plotting.view_surf(\n",
        "      surf_mesh=fsaverage['infl_'+hemi],\n",
        "      surf_map=fsaverage_response,\n",
        "      bg_map=fsaverage['sulc_'+hemi],\n",
        "      threshold=1e-14,\n",
        "      cmap=cmap,\n",
        "      colorbar=colorbar,\n",
        "      title=title\n",
        "      )\n",
        "  return view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eidZIzQBnWKV"
      },
      "outputs": [],
      "source": [
        "# @title Choose subject to visualize\n",
        "subj = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}\n",
        "subj_dir = os.path.join(fmri_dir, f'subj{subj:02d}')\n",
        "\n",
        "lh_fmri = np.load(os.path.join(subj_dir, 'lh_training_fmri.npy'))\n",
        "rh_fmri = np.load(os.path.join(subj_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "print('LH fMRI data shape:')\n",
        "print(lh_fmri.shape)\n",
        "print('(Stimulus images × LH vertices)')\n",
        "\n",
        "print('\\nRH fMRI data shape:')\n",
        "print(rh_fmri.shape)\n",
        "print('(Stimulus images × RH vertices)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5Lq8CZtXluMl"
      },
      "outputs": [],
      "source": [
        "# @title Visualize all vertices on a brain surface map\n",
        "if not quick_execution:\n",
        "  hemisphere = 'left' #@param ['left', 'right']\n",
        "  view = plot_surf_map(subj=subj, hemi=hemisphere)\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaau-XOvkPuB"
      },
      "outputs": [],
      "source": [
        "# @title Visualize a chosen ROI on the surface\n",
        "if not quick_execution:\n",
        "  hemisphere = 'left' #@param [\"left\", \"right\"]\n",
        "  roi = \"ventral\" #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]\n",
        "  view = plot_surf_map(subj=subj, hemi=hemisphere, roi=roi)\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRUsvPO_A12z"
      },
      "source": [
        "### Stimulus images\n",
        "\n",
        "All images come from the [COCO dataset](https://cocodataset.org/#home) of natural scenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUfYsTAp_Vdj"
      },
      "outputs": [],
      "source": [
        "stim_dir = os.path.join(main_dir, 'stimuli')\n",
        "\n",
        "# Create lists will all training and test image file names, sorted\n",
        "img_list = os.listdir(stim_dir)\n",
        "img_list.sort()\n",
        "print('Total n. of images: ' + str(len(img_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RIZIvVsvA_aN"
      },
      "outputs": [],
      "source": [
        "# @title Visualize the fMRI response to selected images\n",
        "if not quick_execution:\n",
        "  img = 0 #@param\n",
        "\n",
        "  #Load the image\n",
        "  img_file = os.path.join(stim_dir, img_list[img])\n",
        "  this_img = Image.open(img_file).convert('RGB')\n",
        "\n",
        "  plt.figure()\n",
        "  plt.axis('off')\n",
        "  plt.imshow(this_img)\n",
        "  plt.title('Image: ' + str(img));\n",
        "\n",
        "  stat_map = lh_fmri[img] if hemisphere == 'left' else rh_fmri[img]\n",
        "\n",
        "  view = plot_surf_map(stat_map=stat_map, subj=subj, hemi=hemisphere)\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KKGlDpTXBBsr"
      },
      "outputs": [],
      "source": [
        "# @title Visualize the fMRI responses of a chosen ROI\n",
        "if not quick_execution:\n",
        "\n",
        "  img = 0 #@param\n",
        "  hemisphere = 'left' #@param ['left', 'right'] {allow-input: true}\n",
        "  roi = \"ventral\" #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "\n",
        "  # Load the image\n",
        "  img_file = os.path.join(stim_dir, img_list[img])\n",
        "  this_img = Image.open(img_file).convert('RGB')\n",
        "\n",
        "  # Plot the image\n",
        "  plt.figure()\n",
        "  plt.axis('off')\n",
        "  plt.imshow(this_img)\n",
        "  plt.title('Training image: ' + str(img+1));\n",
        "\n",
        "  stat_map = lh_fmri[img] if hemisphere == 'left' else rh_fmri[img]\n",
        "\n",
        "  view = plot_surf_map(stat_map=stat_map, subj=subj, hemi=hemisphere, roi=roi)\n",
        "\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "\n",
        "view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiruedt5_WNs"
      },
      "source": [
        "# Feature extraction\n",
        "\n",
        "\n",
        "\n",
        "Here, we're going to extract the feature representations of our stimulus images across different layers of a Deep Convolutional Neural Network (DCNN). These networks process information in a hierarchical fashion, extracting different types of features from the input data. Early DCNN layers extract more low-level visual features (edges, shapes, simple textures), while later layers extract more high-level features (complex patterns, full objects). We can plot these so-called *featuremaps* to inspect how the input is represented across different hierarchical levels of the network:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/WiegerScheurer/rsa_neuroai/main/cnn_layers.png\" width=\"250\"/>\n",
        "\n",
        "This progressive processing of visual abstraction is to some extent analogous to how bottom-up visual sensory information is processed in early visual cortex. Hence, they are well-suited for comparisons with neural activity evoked by these images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiJ_F_agBkUm"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions to obtain a model and extract its activations\n",
        "\n",
        "def get_vision_model(whichmodel):\n",
        "  \"\"\"\n",
        "  Get computer vision model. This function can be\n",
        "  modified to return a custom model.\n",
        "  \"\"\"\n",
        "  if whichmodel in ['alexnet', 'resnet50']:\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', whichmodel)\n",
        "    preprocess = transforms.Compose([\n",
        "      transforms.Resize((224,224)), # resize the images to 224x24 pixels\n",
        "      transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
        "    ])\n",
        "  elif whichmodel=='clip':\n",
        "    model, preprocess = clip.load(\"ViT-B/32\")\n",
        "  else:\n",
        "    raise ValueError(f'Model {whichmodel} unknown!')\n",
        "\n",
        "  return model, preprocess\n",
        "\n",
        "\n",
        "def list_layers(model):\n",
        "    \"\"\"\n",
        "    List all layers of the model with their names.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    for name, mod in model.named_modules():\n",
        "        layers.append(name)\n",
        "    return layers\n",
        "\n",
        "\n",
        "def register_hooks(model, target_layers):\n",
        "    \"\"\"\n",
        "    Register hooks to extract features from specified layers.\n",
        "    \"\"\"\n",
        "    if not isinstance(target_layers, list):\n",
        "      target_layers = [target_layers]\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    def get_hook(name):\n",
        "        def hook(module, input, output):\n",
        "            features[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    for name, layer in model.named_modules():\n",
        "        if name in target_layers:\n",
        "            layer.register_forward_hook(get_hook(name))\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvJoJAcVDbsd"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor():\n",
        "    def __init__(self, modelname, device=device, target_layers=None):\n",
        "        self.modelname = modelname\n",
        "        self.device = device\n",
        "        model, self.preprocess = get_vision_model(modelname)\n",
        "        self.model = model.eval().to(self.device)\n",
        "        if target_layers is not None:\n",
        "            self.create_feature_extractor(target_layers)\n",
        "\n",
        "    def create_feature_extractor(self, target_layers):\n",
        "        self.target_layers = target_layers\n",
        "        self.features = register_hooks(self.model, self.target_layers)\n",
        "\n",
        "    def list_layers(self):\n",
        "        return list_layers(self.model)\n",
        "\n",
        "    def run_model(self, x):\n",
        "\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _ = self.model.encode_image(x) if self.modelname=='clip' else self.model(x)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.run_model(x)\n",
        "        # features = [torch.flatten(l, start_dim=1) for l in self.features.values()]\n",
        "        # features_dict = {}\n",
        "        # for n, l in zip(self.target_layers, self.features.values()):\n",
        "        #     features_dict[n] = torch.flatten(l, start_dim=1).detach().cpu().numpy()\n",
        "        return self.features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSx9am5RDxv-"
      },
      "source": [
        "## Create the feature extractor for a specified model and layer(s).\n",
        "\n",
        "Please note that ```target_layers``` can be a list of layers as well. To find out what layers are available for a given model, you can do the following:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Note that we don't pass the 'target_layers'\n",
        "# as a parameter to add them later!\n",
        "feat_extractor = FeatureExtractor(modelname, device=device)\n",
        "\n",
        "feat_extractor.list_layers()\n",
        "```\n",
        " After choosing one or multiple layers from the list, you can do:\n",
        "\n",
        "```\n",
        "feat_extractor.create_feature_extractor(target_layers)\n",
        "```\n",
        "\n",
        "Up to you to experiment and try different layers and models! You can search the literature for ideas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu4rEycFAODz"
      },
      "source": [
        "---\n",
        "####**Question:**\n",
        "For some research, we might be more interested in a specific range of DCNN layers than in others. Can you think of a situation in which we'd only want to look at earlier layers of a DCNN?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWzX8TJcAEoH"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzFj-d2wmrZt"
      },
      "source": [
        "### Choose model and layer\n",
        "<img src=\"https://raw.githubusercontent.com/WiegerScheurer/rsa_neuroai/main/DCNN_better.png\" width=\"350\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9AN4n5VEANA"
      },
      "outputs": [],
      "source": [
        "modelname = 'alexnet' #@param ['alexnet', 'resnet50', 'clip'] {allow-input: true}\n",
        "target_layers = ['features.2'] #@param {type: 'string'}\n",
        "\n",
        "# Note that target_layers can be a list of strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2ukKULEDumX"
      },
      "outputs": [],
      "source": [
        "# Create the feature extractor\n",
        "feat_extractor = FeatureExtractor(modelname, device=device, target_layers=target_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKKP4SK7mfWe"
      },
      "source": [
        "### Feature extraction function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJfsLtAoFO1R"
      },
      "outputs": [],
      "source": [
        "def extract_features(feature_extractor, dataloader):\n",
        "    features = []\n",
        "    features_dict = {layer: [] for layer in feature_extractor.target_layers}\n",
        "\n",
        "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "        # Extract features\n",
        "        ft = feature_extractor(d)\n",
        "        ft_stacked = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
        "        features.append(ft_stacked.detach().cpu().numpy())\n",
        "\n",
        "        # Append features for each layer separately\n",
        "        for layer, layer_features in ft.items():\n",
        "            # Flatten the features\n",
        "            flattened_features = torch.flatten(layer_features, start_dim=1)\n",
        "            features_dict[layer].append(flattened_features.detach().cpu().numpy())\n",
        "\n",
        "    features_dict = {layer: np.vstack(layer_features) for layer, layer_features in features_dict.items()}\n",
        "\n",
        "    return np.vstack(features), features_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sBzGFYiDA8J"
      },
      "source": [
        "## Create image dataset\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/WiegerScheurer/rsa_neuroai/main/nsd_stimuli2.jpeg\" width=\"350\"/>\n",
        "\n",
        "Here we create a class instance that allows us to load the stimulus images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kybLxlfRCwuA"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Pytorch dataset that loads the images\n",
        "    from our stimulus set.\n",
        "    \"\"\"\n",
        "    def __init__(self, imgs_paths, idxs, transform, device=device):\n",
        "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
        "        self.transform = transform\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image\n",
        "        img_path = self.imgs_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
        "        if self.transform:\n",
        "            img = self.transform(img).to(self.device)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OslrQHsnaYm"
      },
      "source": [
        "### Split fMRI data into training and validation partitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KKSt3UBouCs"
      },
      "source": [
        "For classical RSA, we actually don't need to split our data into training and validation sets; after all, we are only correlating the data, so there is no regression of any kind happening. However, later we are going to do a reweighted RSA (rRSA), and for this we do need a training/validation split. For ease we therefore opt to do the split here already. This way we can also compare the classical RSA with the rRSA on the same amount of stimuli.\n",
        "\n",
        "Feel free however to do the RSA with the training and validation sets together, you just need to append the fMRI and DCNN feature training and validation sets!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GrSXp6go74O"
      },
      "outputs": [],
      "source": [
        "rand_seed = 123 #@param\n",
        "np.random.seed(rand_seed)\n",
        "\n",
        "train_test_split = 90 # @param\n",
        "# Calculate how many stimulus images correspond to 90% of the training data\n",
        "num_train = int(np.round(len(img_list) / 100 * train_test_split))\n",
        "# Shuffle all stimulus images\n",
        "idxs = np.arange(len(img_list))\n",
        "np.random.shuffle(idxs)\n",
        "\n",
        "# Assign 90% of the shuffled stimulus images to the training partition,\n",
        "# and 10% to the test partition\n",
        "idxs_train, idxs_test = idxs[:num_train], idxs[num_train:]\n",
        "\n",
        "print('Training stimulus images: ' + format(len(idxs_train)))\n",
        "print('\\nTest stimulus images: ' + format(len(idxs_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcXTCyHNDJ1X"
      },
      "outputs": [],
      "source": [
        "lh_fmri_train = lh_fmri[idxs_train]\n",
        "lh_fmri_test = lh_fmri[idxs_test]\n",
        "rh_fmri_train = rh_fmri[idxs_train]\n",
        "rh_fmri_test = rh_fmri[idxs_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3le_qcebDIfc"
      },
      "outputs": [],
      "source": [
        "batch_size = 100 #@param\n",
        "# Get the paths of all image files\n",
        "imgs_paths = sorted(list(Path(stim_dir).iterdir()))\n",
        "\n",
        "# The DataLoaders contain the ImageDataset class\n",
        "train_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(imgs_paths, idxs_train, transform=feat_extractor.preprocess),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "test_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(imgs_paths, idxs_test, transform=feat_extractor.preprocess),\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqShGYpeogRW"
      },
      "source": [
        "## Extract features\n",
        "\n",
        "Now, we can extract the feature space representations of our images from the chosen DCNN. The resulting matrices provide an insight into how the visual information is represented at different hierarchical levels of the neural network.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> **Beware:** this can take a while (+-10 min), depending on the chosen model and layer to extract features from.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In the meantime it can be worth briefly scanning over the following papers on brain alignment of DCNNs:\n",
        "\n",
        "*Yamins & DiCarlo (2016): Using goal-driven deep learning models to understand\n",
        "sensory cortex*\n",
        "\n",
        "https://www.cs.jhu.edu/~ayuille/JHUcourses/ProbabilisticModelsOfVisualCognition2020/Lec15/YaminsNature2016.pdf\n",
        "\n",
        "*Güçlü & van Gerven (2015): Deep Neural Networks Reveal a Gradient in the Complexity\n",
        "of Neural Representations across the Ventral Stream*\n",
        "\n",
        "https://www.jneurosci.org/content/jneuro/35/27/10005.full.pdf?ref=https://githubhelp.com\n",
        "\n",
        "*Kriegeskorte (2015): Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing*\n",
        "\n",
        "https://www.annualreviews.org/content/journals/10.1146/annurev-vision-082114-035447\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otYK3oizDjE6"
      },
      "outputs": [],
      "source": [
        "features_train = extract_features(feat_extractor, train_imgs_dataloader)[0]\n",
        "features_test = extract_features(feat_extractor, test_imgs_dataloader)[0]\n",
        "\n",
        "print('\\nTraining images features:')\n",
        "print(features_train.shape)\n",
        "print('(Training stimulus images × features)')\n",
        "\n",
        "print('\\nValidation images features:')\n",
        "print(features_test.shape)\n",
        "print('(Validation stimulus images × features)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yBI3hRVIRDm"
      },
      "source": [
        "# RSA\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hewt4ZVpIYlH"
      },
      "source": [
        "## Constructing RDMs\n",
        "\n",
        "Now that we have extracted the DCNN features, we can use those to compute *representational dissimilarity matrices* (RDMs). Differences in feature representations between stimuli can be compared, giving us a matrix that has a (dis)similarity score between each stimulus pair (and hence a diagonal of zeros). An advantage of these RDMs is that once constructed, they can easily be compared across modalities (e.g. DNN and fMRI response RDMs), regardless of the feature space in which input is provided. This invariance between feature spaces is one of the main reasons why RSA is such a useful method. It allows us to compare modalities that are measured in very distinct ways, producing very different types of data. RDMs eliminate the restrictions imposed by inherent differences between modality-specific data by transforming data into a new space that captures the underlying **representational geometry**.\n",
        "\n",
        "\n",
        "Our data is currently of the shape (stimulus images x fMRI/DCNN features), but will take on the shape (stimulus images x stimulus images) once we compute our RDMs. Each column in our current data tells us how the corresponding image is represented when proccesed by the brain (in voxel activations --> fMRI features) or by a DCNN (in feature maps). When transforming our modality-specific features into RDMs, we are essentially looking at the pairwise differences in stimulus representation.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/WiegerScheurer/rsa_neuroai/main/rdm.png\" width=\"400\"/>\n",
        "\n",
        "Because these are relative differences, RSA is indifferent to the original feature space of our data. You could think of this as comparing the relation between  word pairs in different languages, expressed through distances between word vectors in embedding space. Similariy, we want to retain only the geometrical information encapsulated in the original feature space. To do so, we compute the pairwise distances between how stimuli are represented. We do this for every combination of stimuli, producing our stimuli x stimuli shaped RDM. Each element in this matrix thus represents how different those two stimuli are represented in their original feature space. Consequently, we can compare these representational geometric patterns across modalities.\n",
        "\n",
        "---\n",
        "####**Question:**\n",
        "Can you think of data that RSA allows us to compare, previously either impractical or simply impossible?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i0oB4X754bF"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCr7yNdlGmbg"
      },
      "outputs": [],
      "source": [
        "def create_rdm(activations):\n",
        "    \"\"\"\n",
        "    Function to create an RDM from response activations or features.\n",
        "    \"\"\"\n",
        "    rdm = np.corrcoef(activations)\n",
        "    return rdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ydFi53UJKnAJ"
      },
      "outputs": [],
      "source": [
        "# @title Create Subject RDM\n",
        "rdm_lh_train = create_rdm(lh_fmri_train)\n",
        "rdm_lh_test = create_rdm(lh_fmri_test)\n",
        "rdm_rh_train = create_rdm(rh_fmri_train)\n",
        "rdm_rh_test = create_rdm(rh_fmri_test)\n",
        "\n",
        "print(\"\\nTraining images left and right hemisphere RDM\")\n",
        "print(rdm_lh_train.shape)\n",
        "print(rdm_rh_train.shape)\n",
        "print('(Training stimulus images × Training stimulus images)')\n",
        "\n",
        "print(\"\\nValidation images left and right hemishpere RDM\")\n",
        "print(rdm_lh_test.shape)\n",
        "print(rdm_rh_test.shape)\n",
        "print('(Validation stimulus images × Validation stimulus images)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "I1VLa36gLaTS"
      },
      "outputs": [],
      "source": [
        "# @title Plot subject RDM\n",
        "plot_n_stimuli = 100 # @param\n",
        "hemisphere = \"left\" # @param\n",
        "sns.heatmap(rdm_lh_train[:plot_n_stimuli,:plot_n_stimuli])\n",
        "plt.title(f\"RDM of first {plot_n_stimuli} training stimuli of the {hemisphere} hemisphere\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kFafEwKzKbHB"
      },
      "outputs": [],
      "source": [
        "# @title Create DNN RDM\n",
        "rdm_dnn_train = create_rdm(features_train)\n",
        "rdm_dnn_val = create_rdm(features_test)\n",
        "\n",
        "print(\"\\nTraining images DNN RDM\")\n",
        "print(rdm_dnn_train.shape)\n",
        "print('(Training stimulus images × Training stimulus images)')\n",
        "\n",
        "print(\"\\nValidation images DNN RDM\")\n",
        "print(rdm_dnn_val.shape)\n",
        "print('(Validation stimulus images × Validation stimulus images)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NoHkIphuLL6O"
      },
      "outputs": [],
      "source": [
        "# @title Plot DNN RDM\n",
        "sns.heatmap(rdm_dnn_train[:plot_n_stimuli,:plot_n_stimuli])\n",
        "plt.title(f\"RDM of first {plot_n_stimuli} training stimuli of {modelname} {target_layers}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY09rieVNc4C"
      },
      "source": [
        "## Compare RDMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfQAJ_c7Nxp-"
      },
      "source": [
        "As the RDMs are symmetric along the diagonal, we only need either of the sides (we take the upper triangle). Otherwise, the comparison value we find will be higher than the true correlation. We then flatten this upper triangle to obtain a *representational dissimilarity vector* (RDV). This can conveniently be done using the `triu_indices` function from NumPy.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vTTAg260OGvF"
      },
      "outputs": [],
      "source": [
        "# @title Helper function to vectorize a matrix\n",
        "def triu_vector(rdm):\n",
        "    \"\"\"\n",
        "    Function to extract the upper triangle from a matrix.\n",
        "    \"\"\"\n",
        "    n_stimuli = rdm.shape[0]\n",
        "    triangle = np.triu_indices(n_stimuli, k=1)\n",
        "    rdv = rdm[triangle]\n",
        "\n",
        "    return rdv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_np69cIFVUZ"
      },
      "source": [
        "## Comparison methods\n",
        "Now, all that's left is comparing how these representations of our stimulus images relate to one another. We can compare our RDVs by computing how similar they are using similarity metrics. There are various ways of doing so, depending on different facets of your data and research question. When deciding what similarity metric to use, you should take into account:\n",
        "\n",
        "- **Variance in data scale:** Does the data underlying your RDM/Vs exist on very different scales? if so, do you need to take these differences in scale into account?\n",
        "- **Linearity of relation:** Is it reasonable to expect a linear relationship between your representations?\n",
        "- **Outlier leniency:** How to deal with outliers?\n",
        "- **Interpretability of results:** Not all similarity metrics are as interpretable. Ideally your metric of choice exhibits a local optimal balance between complexity and interpretability.\n",
        "\n",
        "However, the most common and stringent option is computing the cosine similarity between your RDVs. Other options include Pearson's correlation or Spearman's rank correlation without tie correction known as Spearman's $ρ_α$. [Source](https://rsatoolbox.readthedocs.io/en/latest/comparing.html)\n",
        "\n",
        "---\n",
        "####**Question:**\n",
        "Come up with 2 research questions. One for which your comparisons are indifferent to variance in data scale, and one for which you should account for potential differences in data scale. Which similarity metric is best in each situation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WK3CdBR6T4P"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n7mVv7jZ1bk"
      },
      "source": [
        "####**Question:**\n",
        "Try to implement the cosine distance comparison, and one of the correlation methods. For beginners we advise doing the Pearson's correlation, otherwise Spearman's $\\rho_\\alpha$ is more challenging. For an extra challenge, try to do this wihtout using any of the Sklearn or SciPy correlation functions. You can make use of the `triu_vector` helper function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_c2zlEte97J"
      },
      "source": [
        "Cosine similarity is defined as:\n",
        "\n",
        "$$\n",
        "\\text{similarity}(X, Y) = \\frac{\\sum_{i=1}^{n} X_i Y_i}{\\sqrt{\\sum_{i=1}^{n} X_i^2} \\sqrt{\\sum_{i=1}^{n} Y_i^2}}\n",
        "$$\n",
        "\n",
        "Where $X$ and $Y$ are the vectorized RDMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl_X0GFPLs4d"
      },
      "outputs": [],
      "source": [
        "def compare_cosine(rdm1, rdm2):\n",
        "    \"\"\"\n",
        "    Takes two RDMs and computes the cosine similarity between the two.\n",
        "    \"\"\"\n",
        "    # Your code here!\n",
        "    raise NotImplementedError(\"Not yet implemented!\")\n",
        "\n",
        "    return correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "br5vjFWbcChD"
      },
      "outputs": [],
      "source": [
        "# @title Click to see solution\n",
        "def compare_cosine(rdm1, rdm2):\n",
        "    \"\"\"\n",
        "    Takes two RDMs and computes the cosine similarity between the two.\n",
        "    \"\"\"\n",
        "    rdv1 = triu_vector(rdm1)\n",
        "    rdv2 = triu_vector(rdm2)\n",
        "\n",
        "    covariance = np.sum(np.dot(rdv1,rdv2))\n",
        "\n",
        "    std_dev1 = np.sqrt(np.sum(rdv1**2))\n",
        "    std_dev2 = np.sqrt(np.sum(rdv2**2))\n",
        "\n",
        "    correlations = covariance / np.dot(std_dev1, std_dev2)\n",
        "\n",
        "    return correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMq6xs0dbak0"
      },
      "source": [
        "Pearson's correlation is defined as:\n",
        "\n",
        "$$\n",
        "r = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} \\sqrt{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}}\n",
        "$$\n",
        "\n",
        "Where $X$ and $Y$ are the vectorized RDMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrtgDjbZNVIY"
      },
      "outputs": [],
      "source": [
        "def compare_pearson(rdm1, rdm2):\n",
        "    \"\"\"\n",
        "    Takes two RDMs and returns Pearson's correlation between the two.\n",
        "    \"\"\"\n",
        "    # Your code here!\n",
        "    raise NotImplementedError(\"Not yet implemented!\")\n",
        "\n",
        "    return correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "X1wdWViCeY_y"
      },
      "outputs": [],
      "source": [
        "# @title Click to see solution\n",
        "def compare_pearson(rdm1, rdm2):\n",
        "    \"\"\"\n",
        "    Takes two RDMs and returns Pearson's correlation between the two.\n",
        "    \"\"\"\n",
        "    rdv1 = triu_vector(rdm1)\n",
        "    rdv2 = triu_vector(rdm2)\n",
        "\n",
        "    rdv1 = rdv1 - np.mean(rdv1)\n",
        "    rdv2 = rdv2 - np.mean(rdv2)\n",
        "\n",
        "    covariance = np.sum(rdv1 * rdv2)\n",
        "\n",
        "    std_dev1 = np.sqrt(np.sum(rdv1**2))\n",
        "    std_dev2 = np.sqrt(np.sum(rdv2**2))\n",
        "\n",
        "    correlations = covariance / (std_dev1 * std_dev2)\n",
        "\n",
        "    return correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfOOGkZVbiLC"
      },
      "source": [
        "Spearman's $\\rho_\\alpha$ is given by:\n",
        "\n",
        "$$\\frac{12}{n^3 - n}\\sum_{i=1}^{n} (R_{X_i} - \\bar{R}_X) (R_{Y_i} - \\bar{R}_Y)$$\n",
        "\n",
        "Where $R_{X_i}$ and R_{Y_i} are the ranked vectorized RDMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2lSmU8GM0ja"
      },
      "outputs": [],
      "source": [
        "def compare_spearman_rho_alpha(rdm1, rdm2):\n",
        "    \"\"\"\n",
        "    Takes two RDMs and computes Spearman's rho_alpha between the two.\n",
        "    \"\"\"\n",
        "    # Your code here!\n",
        "    raise NotImplementedError(\"Not yet implemented!\")\n",
        "\n",
        "    return correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SEwlWl-xbh58"
      },
      "outputs": [],
      "source": [
        "# @title Click to see solution\n",
        "def compare_spearman_rho_alpha(rdm1, rdm2):\n",
        "    \"\"\"\n",
        "    Takes two RDMs and computes Spearman's rho_alpha between the two.\n",
        "    \"\"\"\n",
        "    rdv1 = triu_vector(rdm1)\n",
        "    rdv2 = triu_vector(rdm2)\n",
        "\n",
        "    rdv1 = rankdata(rdv1)\n",
        "    rdv2 = rankdata(rdv2)\n",
        "\n",
        "    rdv1 = rdv1 - np.mean(rdv1)\n",
        "    rdv2 = rdv2 - np.mean(rdv2)\n",
        "\n",
        "    n_stimuli = rdv1.shape[0]\n",
        "    correlation = 12 * np.dot(rdv1, rdv2) / (n_stimuli ** 3 - n_stimuli)\n",
        "\n",
        "    return correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNca_vP9cGhR"
      },
      "outputs": [],
      "source": [
        "def compare_rdms(rdm1, rdm2, method=\"rho-alpha\"):\n",
        "    if method == 'cosine':\n",
        "        sim = compare_cosine(rdm1, rdm2)\n",
        "    elif method == 'pearson':\n",
        "        sim = compare_pearson(rdm1, rdm2)\n",
        "    elif method == 'rho-alpha':\n",
        "        sim = compare_spearman_rho_alpha(rdm1, rdm2)\n",
        "    else:\n",
        "        raise ValueError('Unknown comparison method requested!')\n",
        "    return sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wYUoUYbZJP_X"
      },
      "outputs": [],
      "source": [
        "# @title Choose comparison method and RDMs\n",
        "method = \"rho-alpha\" # @param [\"cosine\", \"pearson\", \"spearman\", \"rho-alpha\"]\n",
        "rdm1 = rdm_lh_train # @param [\"rdm_lh_train\", \"rdm_rh_train\"]\n",
        "rdm2 = rdm_dnn_train # @param [\"rdm_dnn_train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xL8X8zbg6uX"
      },
      "source": [
        "Now we can use the `compare_rdms` function to compare 2 RDMs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS0Voe6Fbwq0"
      },
      "outputs": [],
      "source": [
        "correlation = compare_rdms(rdm1, rdm2, method)\n",
        "print(f\"The {method} correlation is {correlation}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dASHo4JCqAOK"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions to get ROI and DNN RDMs\n",
        "def get_roi_rdms(subj, roi):\n",
        "\n",
        "    subj_dir = os.path.join(fmri_dir, f'subj{subj:02d}')\n",
        "\n",
        "    lh_fmri = np.load(os.path.join(subj_dir, 'lh_training_fmri.npy'))\n",
        "    rh_fmri = np.load(os.path.join(subj_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "    lh_fmri_train = lh_fmri[idxs_train]\n",
        "    lh_fmri_test = lh_fmri[idxs_test]\n",
        "    rh_fmri_train = rh_fmri[idxs_train]\n",
        "    rh_fmri_test = rh_fmri[idxs_test]\n",
        "\n",
        "    # Get subject ROI data\n",
        "    lh_mask = get_roi_mask(roi, 'lh', subj)[0][:lh_fmri.shape[1]]\n",
        "    lh_roi_train = lh_fmri_train[:,np.where(lh_mask)[0]]\n",
        "    lh_roi_test = lh_fmri_test[:,np.where(lh_mask)[0]]\n",
        "\n",
        "    rh_mask = get_roi_mask(roi, 'rh', subj)[0][:lh_fmri.shape[1]]\n",
        "    rh_roi_train = rh_fmri_train[:,np.where(rh_mask)[0]]\n",
        "    rh_roi_test = rh_fmri_test[:,np.where(rh_mask)[0]]\n",
        "\n",
        "    # Create ROI RDMs\n",
        "    rdm_lh_roi_train = create_rdm(lh_roi_train)\n",
        "    rdm_lh_roi_test = create_rdm(lh_roi_test)\n",
        "    rdm_rh_roi_train = create_rdm(rh_roi_train)\n",
        "    rdm_rh_roi_test = create_rdm(rh_roi_test)\n",
        "\n",
        "    return rdm_lh_roi_train, rdm_lh_roi_test, rdm_rh_roi_train, rdm_rh_roi_test\n",
        "\n",
        "def get_dnn_rdms(model, layers):\n",
        "    feat_extractor = FeatureExtractor(model, device=device, target_layers=layers)\n",
        "\n",
        "    _, features_train_dict = extract_features(feat_extractor, train_imgs_dataloader)\n",
        "    _, features_test_dict = extract_features(feat_extractor, test_imgs_dataloader)\n",
        "\n",
        "    rdm_dnn_train = np.zeros((len(idxs_train), len(idxs_train), len(layers)))\n",
        "    rdm_dnn_test = np.zeros((len(idxs_test), len(idxs_test), len(layers)))\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        rdm_dnn_train[:,:,i] = create_rdm(features_train_dict[layer])\n",
        "        rdm_dnn_test[:,:,i] = create_rdm(features_test_dict[layer])\n",
        "\n",
        "    return rdm_dnn_train, rdm_dnn_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mc_W42Ao0Hu"
      },
      "outputs": [],
      "source": [
        "lh_train, lh_test, rh_train, rh_test = get_roi_rdms(1, \"ventral\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL_zryFbp099"
      },
      "outputs": [],
      "source": [
        "compare_rdms(lh_train, rh_train, method=\"rho-alpha\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTL4vVHU7WHP"
      },
      "source": [
        "## Compare multiple layers and ROIs\n",
        "\n",
        "Now it is time to explore the true flexibility of RSA. We can make comparisons using different DNN layers and ROIs. As long as the RDMs are of the same shape, virtually any modality could be used in the comparison!\n",
        "\n",
        "We will here choose one model and create and RDM for multiple layers of this model. We will then correlate each of these RDMs to both the \"early\" and \"ventral\" ROIs, similar to the first tutorial.\n",
        "\n",
        "Feel free to experiment with different DNNs, layers and ROIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4QElW5AzJK36"
      },
      "outputs": [],
      "source": [
        "# @title Choose model and layers\n",
        "modelname = 'alexnet' #@param ['alexnet', 'resnet50', 'clip'] {allow-input: true}\n",
        "target_layers = [\"features.2\", \"features.5\", \"features.7\", \"features.9\", \"features.12\"] #@param {type: 'string'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOmU1_aV7u-d"
      },
      "outputs": [],
      "source": [
        "rdm_dnn_train, rdm_dnn_test = get_dnn_rdms(modelname, target_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KOs0aF3DRuQW"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions to plot RSA results\n",
        "def barplot(ax, data, x, y, hue, title=None, noise_ceilings=None, ylim=None):\n",
        "    \"\"\"\n",
        "    Function to show a barplot with an additional (optional) noise ceiling.\n",
        "    \"\"\"\n",
        "    sns.barplot(ax=ax, x=x, hue=hue, y=y, errorbar=('ci', 68), data=data)\n",
        "\n",
        "    if ylim is not None:\n",
        "        ax.set_ylim(ylim)\n",
        "\n",
        "    if noise_ceilings is not None:\n",
        "        upper_lim = max([nc[1] for nc in noise_ceilings]) + 0.1\n",
        "        ax.set_ylim(0, upper_lim)\n",
        "\n",
        "        bar_width = [b.get_width() for b in ax.patches if b.get_width() != 0][0]\n",
        "\n",
        "        noiseceil_idx = 0\n",
        "\n",
        "        for i, roi in enumerate(data['roi'].unique()):\n",
        "\n",
        "            nc1 = noise_ceilings[noiseceil_idx]\n",
        "            left1 = i - bar_width\n",
        "            bottom1 = nc1[0]\n",
        "\n",
        "            ax.add_patch(plt.Rectangle((left1, bottom1), bar_width-0.01, nc1[1] - nc1[0], color='grey', alpha=0.3))\n",
        "\n",
        "            nc2 = noise_ceilings[noiseceil_idx + 1]\n",
        "            left2 = i\n",
        "            bottom2 = nc2[0]\n",
        "\n",
        "            ax.add_patch(plt.Rectangle((left2, bottom2), bar_width-0.01, nc1[1] - nc1[0], color='grey', alpha=0.3))\n",
        "\n",
        "            noiseceil_idx += 2\n",
        "\n",
        "    ax.set_title(title)\n",
        "\n",
        "def plot_all_layers(data, x='roi', y='mean_corr', hue='hemisphere', noise_ceilings=None, n_cols=3):\n",
        "    \"\"\"\n",
        "    Function to create barplots for each layer in a single figure with a common y-axis.\n",
        "    \"\"\"\n",
        "    layers = data['layer'].unique()\n",
        "    n_layers = len(layers)\n",
        "    n_rows = (n_layers + n_cols - 1) // n_cols  # Calculate the number of rows needed\n",
        "\n",
        "    # Determine the global y-axis limits\n",
        "    global_min = data[y].min()\n",
        "    global_max = data[y].max()\n",
        "    ylim = (global_min, global_max)\n",
        "\n",
        "    # Create a grid of subplots\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows), sharex=True)\n",
        "    axes = axes.flatten()  # Flatten in case we have more than 1 row\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        ax = axes[i]\n",
        "        layer_data = data[data['layer'] == layer]\n",
        "        layer_title = f'Layer: {layer}'\n",
        "\n",
        "        barplot(ax, layer_data, x=x, y=y, hue=hue, title=layer_title, noise_ceilings=noise_ceilings, ylim=ylim)\n",
        "\n",
        "    # Remove any unused subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sX1TV7lEJaQ7"
      },
      "outputs": [],
      "source": [
        "# @title Choose ROIs\n",
        "roi_1 = 'early' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "roi_2 = 'ventral' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKjoFz1SourQ"
      },
      "outputs": [],
      "source": [
        "rsa_df = []\n",
        "\n",
        "for subj in tqdm(range(1, 9)):\n",
        "    roi_1_lh_train, _, roi_1_rh_train, _ = get_roi_rdms(subj, roi_1)\n",
        "    roi_2_lh_train, _, roi_2_rh_train, _ = get_roi_rdms(subj, roi_2)\n",
        "\n",
        "    for layer in range(5):\n",
        "        rsa_df.append({\n",
        "            'subject': f'subj-{subj:02d}',\n",
        "            'layer': f'{target_layers[layer]}',\n",
        "            f'mean_corr': compare_rdms(rdm_dnn_train[:,:,layer], roi_1_lh_train),\n",
        "            'hemisphere': 'left',\n",
        "            'roi': roi_1\n",
        "        })\n",
        "        rsa_df.append({\n",
        "            'subject': f'subj-{subj:02d}',\n",
        "            'layer': f'{target_layers[layer]}',\n",
        "            f'mean_corr': compare_rdms(rdm_dnn_train[:,:,layer], roi_1_rh_train),\n",
        "            'hemisphere': 'right',\n",
        "            'roi': roi_1\n",
        "        })\n",
        "        rsa_df.append({\n",
        "            'subject': f'subj-{subj:02d}',\n",
        "            'layer': f'{target_layers[layer]}',\n",
        "            f'mean_corr': compare_rdms(rdm_dnn_train[:,:,layer], roi_2_lh_train),\n",
        "            'hemisphere': 'left',\n",
        "            'roi': roi_2\n",
        "        })\n",
        "        rsa_df.append({\n",
        "            'subject': f'subj-{subj:02d}',\n",
        "            'layer': f'{target_layers[layer]}',\n",
        "            f'mean_corr': compare_rdms(rdm_dnn_train[:,:,layer], roi_2_rh_train),\n",
        "            'hemisphere': 'right',\n",
        "            'roi': roi_2\n",
        "        })\n",
        "\n",
        "rsa_df = pd.DataFrame(rsa_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HBYdmDD4Ltsh"
      },
      "outputs": [],
      "source": [
        "# @title Plot results\n",
        "plot_all_layers(rsa_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFzVzwaMYNoM"
      },
      "source": [
        "## Noise ceiling\n",
        "\n",
        "Similar to yesterday's tutorial, we will calculate the noise ceiling across participants [source](https://www.johancarlin.com/understanding-noise-ceiling-metrics-rsa-compared-to-spearman-brown.html). For RSA, this is relatively straightforward, as we can use the subject RDMs in place of the activations. Recall the explanation from Tutorial 1:\n",
        "\n",
        "- We estimate a *lower noise ceiling* by correlating the activations of a held-out subject to the mean activations of the remaining $N-1$ subjects. This is an under-estimate of the true maximum correlation.\n",
        "\n",
        "- And we estimate an *upper noise ceiling* by correlating the activations of each subject to the mean activations of all subjects, *including* that subject. This is an over-estimate of the true maximum correlation.\n",
        "\n",
        "We estimate both of these across all participants, and take the average. The true noise ceiling will lie somewhere in between these two extremes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXcsub9Sianx"
      },
      "outputs": [],
      "source": [
        "def lower_noise_ceiling(rdms):\n",
        "    \"\"\"Returns the lower noise ceiling given a list of subject RDMS.\"\"\"\n",
        "    num_subjs = len(rdms)\n",
        "    lnc = 0.0\n",
        "\n",
        "    for i in range(num_subjs):\n",
        "        subj_rdm = rdms[i]\n",
        "        subj_rdm = subj_rdm[np.triu_indices(subj_rdm.shape[0],k=1)]\n",
        "        rdm_subj_removed = np.delete(rdms, i, axis=0)\n",
        "\n",
        "        mean_subj_rdm = np.mean(rdm_subj_removed, axis=0)\n",
        "        mean_subj_rdm = mean_subj_rdm[np.triu_indices(mean_subj_rdm.shape[0],k=1)]\n",
        "        lnc += pearsonr(subj_rdm, mean_subj_rdm)[0]\n",
        "\n",
        "    lnc = lnc / num_subjs\n",
        "    return lnc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3za3kevViU5m"
      },
      "outputs": [],
      "source": [
        "def upper_noise_ceiling(rdms):\n",
        "    \"\"\"Returns the upper noise ceiling given a list of subject RDMS.\"\"\"\n",
        "    num_subjs = len(rdms)\n",
        "    unc = 0.0\n",
        "    mean_subj_rdm = np.mean(rdms, axis=0)\n",
        "    mean_subj_rdm = mean_subj_rdm[np.triu_indices(mean_subj_rdm.shape[0],k=1)]\n",
        "\n",
        "    for i in range(num_subjs):\n",
        "        subj_rdm = rdms[i]\n",
        "        subj_rdm = subj_rdm[np.triu_indices(subj_rdm.shape[0],k=1)]\n",
        "        unc += pearsonr(subj_rdm, mean_subj_rdm)[0]\n",
        "\n",
        "    unc = unc / num_subjs\n",
        "    return unc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U98_D2TxfIxo"
      },
      "outputs": [],
      "source": [
        "def compute_noise_ceiling(roi, hemi):\n",
        "    \"\"\"Compute the noise ceiling for a ROI and hemisphere.\"\"\"\n",
        "    all_rdms = []\n",
        "    for subj in range(1, 9):\n",
        "        if hemi == \"lh\":\n",
        "            rdm, _, _, _ = get_roi_rdms(subj, roi)\n",
        "        elif hemi == \"rh\":\n",
        "            _, _, rdm, _ = get_roi_rdms(subj, roi)\n",
        "\n",
        "        all_rdms.append(rdm)\n",
        "\n",
        "    lower = lower_noise_ceiling(all_rdms)\n",
        "    upper = upper_noise_ceiling(all_rdms)\n",
        "\n",
        "    return (lower, upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fV7KUpJ1cEXS"
      },
      "outputs": [],
      "source": [
        "# @title Compute the noise ceiling for each ROI and hemisphere\n",
        "all_noiseceilings = []\n",
        "for roi in [roi_1, roi_2]:\n",
        "    for hemi in ['lh', 'rh']:\n",
        "        this_nc = compute_noise_ceiling(roi, hemi)\n",
        "        print(roi, hemi, this_nc)\n",
        "        all_noiseceilings.append(this_nc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "t8IZ_PbPlecl"
      },
      "outputs": [],
      "source": [
        "# @title Plot results with noise ceilings\n",
        "plot_all_layers(rsa_df, noise_ceilings=all_noiseceilings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bINHNWXmcxeR"
      },
      "source": [
        "---\n",
        "####**Question:**\n",
        "What conlusions can we draw from these results? What might be possibilities to improve the alignment?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF2C7vAOcxeb"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWgkSWA1Qn0n"
      },
      "source": [
        "# Reweighted RSA\n",
        "\n",
        "Classical RSA gives equal value to all features. This might lead to underestimating the actual correlation between the neural data and the DNN activations. One way to improve this alignment is to do a reweighting of the features. [This paper](https://www.sciencedirect.com/science/article/pii/S105381192200413X) provides an in depth explanation of reweighted RSA and its possibilities.\n",
        "\n",
        "We can reweight the features using a regression, a common type of regression to use for this is a ridge regression (there it is again!) with cross-validation for the parameters.\n",
        "To remind you what the ridge optimization problem looks like:\n",
        "\n",
        "$$w = \\arg\\min_{w}||Xw - y||^2+\\alpha||w||^2$$\n",
        "\n",
        "We provide a search space for the `alpha` hyperparameter, but feel free to adjust it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkS9lnJrQnGP"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions for the ridge regression\n",
        "def get_preds_from_RDM(rdm):\n",
        "    '''\n",
        "    This function takes in an RDM and returns a feature matrix\n",
        "    The RDM is in a shape of (n_stimuli, n_stimuli, n_layers)\n",
        "    '''\n",
        "    # Get shape of flattened upper triangle\n",
        "    rdv = triu_vector(rdm[:,:,0])\n",
        "\n",
        "    features = np.zeros((rdv.shape[0], rdm.shape[2])) # Length of flattened upper triangle x number of layers\n",
        "\n",
        "    # Fill in predictors\n",
        "    for l in range(rdm.shape[2]):\n",
        "        features[:,l] = triu_vector(rdm[:,:,l])\n",
        "\n",
        "    # Standardization step\n",
        "    scaler = StandardScaler()\n",
        "    features_s = np.zeros((features.shape))\n",
        "    for l in range(features.shape[1]):\n",
        "        feats_s = scaler.fit_transform(features[:,l].reshape(-1,1))\n",
        "        features_s[:,l] = feats_s[:,0]\n",
        "\n",
        "    return features_s\n",
        "\n",
        "def fit_RIDGE(subj_rdm, dnn_rdm):\n",
        "    \"\"\"Fit a ridge regression to the RDMs\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    alphas = np.logspace(-1, 5, 7)\n",
        "    all_betas = np.zeros(dnn_rdm.shape[1])\n",
        "    rdv = triu_vector(subj_rdm)\n",
        "    rdv = scaler.fit_transform(rdv.reshape(-1,1))\n",
        "    model = RidgeCV(alphas=alphas, scoring='explained_variance').fit(dnn_rdm, rdv)\n",
        "    all_betas = model.coef_\n",
        "\n",
        "    return all_betas\n",
        "\n",
        "def reweighted_rsa(dnn_train, dnn_test, subj_train, subj_test):\n",
        "\n",
        "    feats = get_preds_from_RDM(dnn_train)\n",
        "\n",
        "    # Regression betas\n",
        "    betas = fit_RIDGE(subj_train, feats)[0]\n",
        "\n",
        "    scaler=StandardScaler()\n",
        "    subj_test_rdv = triu_vector(subj_test)\n",
        "    subj_test_rdv = scaler.fit_transform(subj_test_rdv.reshape(-1,1))\n",
        "\n",
        "    pred_rw = get_preds_from_RDM(dnn_test) @ betas\n",
        "\n",
        "    corr = pearsonr(pred_rw.flatten(), subj_test_rdv.flatten())[0]\n",
        "\n",
        "    return corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uManRQm3sNCm"
      },
      "outputs": [],
      "source": [
        "# @title Choose ROIs\n",
        "roi_1 = 'early' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "roi_2 = 'ventral' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "\n",
        "rrsa_df = []\n",
        "\n",
        "for subj in tqdm(range(1, 9)):\n",
        "    roi_1_lh_train, roi_1_lh_test, roi_1_rh_train, roi_1_rh_test = get_roi_rdms(subj, roi_1)\n",
        "    roi_2_lh_train, roi_2_lh_test, roi_2_rh_train, roi_2_rh_test = get_roi_rdms(subj, roi_2)\n",
        "\n",
        "    lh_rw1 = reweighted_rsa(rdm_dnn_train, rdm_dnn_test, roi_1_lh_train, roi_1_lh_test)\n",
        "    rh_rw1 = reweighted_rsa(rdm_dnn_train, rdm_dnn_test, roi_1_rh_train, roi_1_rh_test)\n",
        "    lh_rw2 = reweighted_rsa(rdm_dnn_train, rdm_dnn_test, roi_2_lh_train, roi_2_lh_test)\n",
        "    rh_rw2 = reweighted_rsa(rdm_dnn_train, rdm_dnn_test, roi_2_rh_train, roi_2_rh_test)\n",
        "\n",
        "    rrsa_df.append({\n",
        "        'subject': f'subj-{subj:02d}',\n",
        "        'layer': 'reweighted',\n",
        "        f'mean_corr': lh_rw1,\n",
        "        'hemisphere': 'left',\n",
        "        'roi': roi_1\n",
        "    })\n",
        "    rrsa_df.append({\n",
        "        'subject': f'subj-{subj:02d}',\n",
        "        'layer': 'reweighted',\n",
        "        f'mean_corr': rh_rw1,\n",
        "        'hemisphere': 'right',\n",
        "        'roi': roi_1\n",
        "    })\n",
        "    rrsa_df.append({\n",
        "        'subject': f'subj-{subj:02d}',\n",
        "        'layer': 'reweighted',\n",
        "        f'mean_corr': lh_rw2,\n",
        "        'hemisphere': 'left',\n",
        "        'roi': roi_2\n",
        "    })\n",
        "    rrsa_df.append({\n",
        "        'subject': f'subj-{subj:02d}',\n",
        "        'layer': 'reweighted',\n",
        "        f'mean_corr': rh_rw2,\n",
        "        'hemisphere': 'right',\n",
        "        'roi': roi_2\n",
        "    })\n",
        "\n",
        "rrsa_df = pd.DataFrame(rrsa_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fmDTNvnww6SS"
      },
      "outputs": [],
      "source": [
        "# @title Plot results with noise ceilings\n",
        "plot_all_layers(rrsa_df, noise_ceilings=all_noiseceilings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPtbro2VsDTF"
      },
      "source": [
        "---\n",
        "####**Question:**\n",
        "Can you see any improvement compared to the classical RSA? How could this be explained?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQA09qeksDTI"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqtGmJ9sT7Sq"
      },
      "source": [
        "# Multidimensional scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-IIus7Pqim7"
      },
      "source": [
        "With *multidimensional scaling* (MDS) we can force our RDMs to populate a smaller space (usually two dimensions). This way, we can visualize whether representations of similar images are actually closer together, and thus more similarly represented. This can help us explore te data, potentially revealing patterns we might not have thought of.\n",
        "\n",
        "To not make it to crowded, we use the test set for the visualization. Feel free to change the data used!\n",
        "\n",
        "Although the COCO images themselves are unlabeled, the segmented object in the images are categorized. We can use these categories and supercategories to filter the images on whether they contain an object or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oFA-0zWaKxX1"
      },
      "outputs": [],
      "source": [
        "# @title Get object label dataframe\n",
        "df = pd.read_csv(main_dir + \"/additional_data/stimulus_data.csv\")\n",
        "df['nsd_id'] = df['image_name'].str.extract(r'(nsd\\d+)')\n",
        "shared_ids = [id.split('_')[-1].split('.')[0].replace('-', '') for id in img_list]\n",
        "shared_ids = [f\"nsd{int(id[3:]) + 1:05d}\" for id in shared_ids]\n",
        "df = df[df['nsd_id'].isin(shared_ids)]\n",
        "df = df.sort_values(by='nsd_id').reset_index()\n",
        "df = df.loc[sorted(idxs_test)].reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bG_EVshqX0Rv"
      },
      "outputs": [],
      "source": [
        "# @title Select super category\n",
        "super_category = 'vehicle' # @param ['person', 'sports', 'animal', 'vehicle', 'kitchen', 'appliance', 'electronic', 'food', 'furniture', 'indoor', 'outdoor', 'accessory']\n",
        "object_ids = df.index[df['coco_supercategs'].apply(lambda x: super_category in x)].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGFLkQyDsTJ4"
      },
      "outputs": [],
      "source": [
        "# @title Plot MDS\n",
        "mds = MDS(dissimilarity='precomputed', n_components=2, normalized_stress='auto')\n",
        "mds_R = mds.fit_transform(rdm_dnn_test[:,:,0])\n",
        "print(\"Shape of mds_R:\", mds_R.shape)\n",
        "\n",
        "idxs = np.argsort(np.array(shared_ids)[idxs_test])\n",
        "is_object = np.array([i in object_ids for i in range(len(mds_R))])[np.argsort(idxs)]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.grid()\n",
        "plt.scatter(mds_R[is_object, 0], mds_R[is_object, 1], c='orange', label='object')\n",
        "plt.scatter(mds_R[~is_object, 0], mds_R[~is_object, 1], c='blue', label='not object')\n",
        "plt.legend()\n",
        "plt.xlabel('MDS component 1', fontsize=20)\n",
        "plt.ylabel('MDS component 2', fontsize=20)\n",
        "xlim = plt.gca().get_xlim()\n",
        "ylim = plt.gca().get_ylim()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjIVYAR0fVS4"
      },
      "outputs": [],
      "source": [
        "# @title Plot MDS with images\n",
        "from PIL import ImageOps\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "plt.grid()\n",
        "\n",
        "images = [img_list[i] for i in idxs_test]\n",
        "images = [Image.open(os.path.join(stim_dir, img_name)) for img_name in images]\n",
        "\n",
        "# Function to create an AnnotationBbox with an image and a colored border\n",
        "def image_annotation(ax, xy, image, zoom=0.1, border_color='red'):\n",
        "    # Add a border to the image\n",
        "    border_size = max(1, int(max(image.size) * 0.05))  # Border is 5% of the image size\n",
        "    image_with_border = ImageOps.expand(image, border=border_size, fill=border_color)\n",
        "\n",
        "    imagebox = OffsetImage(image_with_border, zoom=zoom)\n",
        "    ab = AnnotationBbox(imagebox, xy, frameon=False, pad=0.3)\n",
        "    ax.add_artist(ab)\n",
        "\n",
        "# Add images to the plot with a colored border\n",
        "for i in range(len(mds_R)):\n",
        "    xy = (mds_R[i, 0], mds_R[i, 1])\n",
        "    border_color = 'blue' if is_object[i] else 'red'\n",
        "    image_annotation(ax, xy, images[i], zoom=0.08 if is_object[i] else 0.08, border_color=border_color)\n",
        "\n",
        "plt.xlim(xlim)\n",
        "plt.ylim(ylim)\n",
        "\n",
        "plt.xlabel('MDS component 1', fontsize=20)\n",
        "plt.ylabel('MDS component 2', fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3VVIzxMKlYD"
      },
      "source": [
        "---\n",
        "####**Question:**\n",
        "Looking at our MDS results, we don't see a very distinct pattern between the two image categories used. Provide **two** reasons why this might be the case in our current analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp1OSxYkMQpj"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVMlt6COMqMn"
      },
      "source": [
        "___\n",
        "The end of part 1\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x34vkLhKHD_v"
      },
      "source": [
        "# Tutorial Day 2 - SPoSE\n",
        "\n",
        "by Giacomo Aldegheri\n",
        "\n",
        "\\\n",
        "\\\n",
        "In this tutorial, we will look at **SPoSE** (Sparse Positive Object Similarity Embeddings) a model proposed by Martin Hebart and colleagues in a [2020 paper](https://www.nature.com/articles/s41562-020-00951-3) and then in a [follow-up paper](https://elifesciences.org/articles/82580) to estimate the dimensions underlying human judgments of image similarities. What does that mean?\n",
        "\n",
        "They ran a large-scale online behavioral study, in which they simply showed, on each trial, 3 images, and asked participants to indicate which was the odd one out (the image most dissimilar to the others):\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1zONI4kRZmsP0jlO-tNgPYy46EhVsHXVL' width=400>\n",
        "\n",
        "They did this for 1854 object concepts (from the [THINGS database](https://things-initiative.org/)) and 1.46 million trials.\n",
        "\n",
        "Using this data, they tried to understand the dimensions underlying subjects' similarity judgments. They used the following procedure:\n",
        "\n",
        "\\\n",
        "<img src='https://drive.google.com/uc?id=1zPHHATd117y1YWGSs4FbRoD9FI5TWGjh' width=750>\n",
        "\n",
        "\\\n",
        "In essence, they pre-specified a number of embedding dimensions (e.g. 40) and created a simple linear model with a `1854 x 40` (n. concepts x n. dimensions) weight matrix. They fed each concept as a 1854-dimensional one-hot vector to the model, and transformed it into a 40-dimensional embedding.\n",
        "\n",
        "The embeddings were extracted for each of the three items in a triplet (**step 1** in the figure above), and their pairwise similarity was computed with a dot product (**step 2**). These dot products were turned into choice probabilities using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function) (**step 3**), and the model's choice (pair with the highest probability) was compared to a human subject's choice (**step 4**).\n",
        "\n",
        "The idea is that among the three pairs, the highest probability (pairwise similarity) should be assigned to the pair that does *not* include the odd-one-out. For example, if from the triplet `glass, bottle, car` I judge the car to be the odd-one-out, the model should predict the pair `glass, bottle` to have a higher similarity than `glass, car` and `bottle, car`. The distance ([cross-entropy loss](https://en.wikipedia.org/wiki/Cross-entropy)) between the model's choice and the correct choice was then [backpropagated](https://en.wikipedia.org/wiki/Backpropagation) to the weights, making them increasingly informative about subjects' judgments.\n",
        "\n",
        "They also put two additional constraints on the weights:\n",
        "\n",
        "- They had to be **positive**: the intuition is that each weight should reflect the presence, or absence, of a given feature. E.g. an animal can be more or less furry, but it can't be negative furry.\n",
        "\n",
        "- They had to be **sparse**: (for any given input, most features' activations should be 0) when a lot of features are present for each object, they are usually not very interpretable. Only a few features should be active for any given object.\n",
        "\n",
        "Both of these constraints were informed using special loss functions, as we will see below.\n",
        "\n",
        "The resulting features turn out to reflect interpretable concepts, that reflect the dimensions along which people's internal representations of objects are organized.\n",
        "\n",
        "\\\n",
        "With all that in mind, time to dive into the code! This tutorial closely follows the [official implementation](https://github.com/ViCCo-Group/SPoSE).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cQrYT1ZfO4R"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHK3ehScGhpJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Tuple\n",
        "import random\n",
        "\n",
        "rand_seed = 123\n",
        "random.seed(rand_seed)\n",
        "np.random.seed(rand_seed)\n",
        "torch.manual_seed(rand_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVU7-o4IfSfQ"
      },
      "source": [
        "## Set device\n",
        "\n",
        "If possible, use the GPU for much faster computation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5gzeeAfABVQ"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' # @param ['cuda', 'cpu']\n",
        "if device == 'cuda':\n",
        "  assert torch.cuda.is_available(), 'GPU not available! Please select a GPU runtime or use CPU.'\n",
        "device = torch.device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVp38XEPfY8W"
      },
      "source": [
        "## Mount Google Drive\n",
        "\n",
        "You should have already added a shortcut to the data folder in your Google Drive during yesterday's tutorial. If not, this is how you do it:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=15TNjV__sWCcnBRlxbXNbJfpidx-C6nrk' width=500>\n",
        "\n",
        "Go to the [folder](https://drive.google.com/drive/folders/1AjDOejWLjfXGkr-hK07SZJ_4ni1nypjw?usp=sharing), right click on its name, and select `Organize -> Add shortcut`. It will add a shortcut to your own Google Drive without the need to copy any data or occupy any storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufbBY0c1Hgud"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/UvA_encodingtutorial/SPoSE/triplet_dataset/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcRxBpA8dR5K"
      },
      "source": [
        "## Specify directory to save trained model parameters\n",
        "\n",
        "After we have trained the model (as well as during training, to see how the weights evolve) we want to save the model's weights.\n",
        "\n",
        "This should be on your own Google Drive, not in the shared folder, where you don't have writing permissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOyz2YDudG2a"
      },
      "outputs": [],
      "source": [
        "log_dir = 'SPoSE_weights' #@param {type: 'string'}\n",
        "log_dir = f'/content/drive/MyDrive/{log_dir}/'\n",
        "if not os.path.isdir(log_dir):\n",
        "  os.makedirs(log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Dc9acn-vX8TM"
      },
      "outputs": [],
      "source": [
        "# @title Utility functions\n",
        "\n",
        "def load_triplets(partition:str, data_dir=data_dir):\n",
        "  if partition == 'train':\n",
        "    fname = 'trainset.txt'\n",
        "    n_items = 500000\n",
        "  elif partition == 'val':\n",
        "    fname = 'validationset.txt'\n",
        "    n_items = 20000\n",
        "  elif partition == 'test':\n",
        "    fname = 'testset1.txt'\n",
        "    n_items = 10000\n",
        "\n",
        "  triplets = np.loadtxt(os.path.join(data_dir, fname))\n",
        "\n",
        "  return torch.from_numpy(triplets[:n_items]).type(torch.LongTensor)\n",
        "\n",
        "def accuracy_(probas:np.ndarray) -> float:\n",
        "    choices = np.where(probas.mean(axis=1) == probas.max(axis=1), -1, np.argmax(probas, axis=1))\n",
        "    acc = np.where(choices == 0, 1, 0).mean()\n",
        "    return acc\n",
        "\n",
        "def choice_accuracy(anchor:torch.Tensor, positive:torch.Tensor, negative:torch.Tensor) -> float:\n",
        "    similarities  = compute_similarities(anchor, positive, negative)\n",
        "    probas = F.softmax(torch.stack(similarities, dim=-1), dim=1).detach().cpu().numpy()\n",
        "    return accuracy_(probas)\n",
        "\n",
        "def filter_nonneg(W, threshold=0.1):\n",
        "  W = W*(W>threshold)\n",
        "  return W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2pyJ34pi53K"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Here, to speed up computations, we will only use a subset of the available. Specifically, this dataset contains 4.12M triplets, but we will only use 500K for training. It's less than 1/8 of the data, but we will see that it works quite well! For validation, we will use 20K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWqbH_-QZH6s"
      },
      "outputs": [],
      "source": [
        "train_triplets = load_triplets('train')\n",
        "val_triplets = load_triplets('val')\n",
        "\n",
        "print('Number of training samples:', len(train_triplets))\n",
        "print('Number of validation samples:', len(val_triplets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufguSJ6h3jf6"
      },
      "source": [
        "The data contains 1854 concepts, as we can verify:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_6FQOUOI255"
      },
      "outputs": [],
      "source": [
        "n_items = len(torch.unique(train_triplets))\n",
        "print('N. unique concepts:', n_items)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5uvGlc75YvC"
      },
      "source": [
        "Let's create a dataset to feed the triplets to our model. First, the unique concept IDs need to be coded as one-hot vectors (1854-dimensional, with 1 for the current concept's entry and 0 elsewhere)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRbcaHpcSpu2"
      },
      "outputs": [],
      "source": [
        "def encode_as_onehot(I:torch.Tensor, triplets:torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"encode item triplets as one-hot-vectors\"\"\"\n",
        "    return I[triplets.flatten(), :]\n",
        "\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "\n",
        "    def __init__(self, n_items:int, dataset:torch.Tensor):\n",
        "        self.I = torch.eye(n_items)\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx:int) -> torch.Tensor:\n",
        "        sample = encode_as_onehot(self.I, self.dataset[idx])\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkY3NlJm9ntq"
      },
      "source": [
        "## Model\n",
        "\n",
        "Finally, time to code the actual model! As you can see, it's really simple. Just a single linear layer (only weights, no biases), with the number of concepts as input size and the number of embedding dimensions as output size.\n",
        "\n",
        "**EXERCISE:** fill in the code for the single linear layer in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl082nKC9bjL"
      },
      "outputs": [],
      "source": [
        "class SPoSE(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                in_size:int,\n",
        "                out_size:int,\n",
        "                init_weights:bool=True,\n",
        "                ):\n",
        "        super(SPoSE, self).__init__()\n",
        "        self.in_size = in_size\n",
        "        self.out_size = out_size\n",
        "        # YOUR CODE HERE\n",
        "        #self.fc = ...\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "        return self.fc(x)\n",
        "\n",
        "    def _initialize_weights(self) -> None:\n",
        "        mean, std = .1, .01\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(mean, std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTNNmTvI-7vD"
      },
      "source": [
        "## Loss functions\n",
        "\n",
        "And now one of the most important ingredients: the loss functions. First we code the two regularizers: the L1 regularization to enforce sparsity, and the positivity penalty to enforce positive weights.\n",
        "\n",
        "- For **sparsity**, we use the [L1 loss](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261): $\\sum_{i=1}^n|\\textbf{W}_i|$ for each weight $\\textbf{W}_i$.\n",
        "\n",
        "- For **positivity**, we use the sum of all weights less than 0 as a loss term: $\\sum_{i=1}^{n}\\operatorname{ReLU}(-\\textbf{W}_i)$ for each weight $\\textbf{W}_i$.\n",
        "\n",
        "\\\n",
        "**EXERCISE:** fill in the code for the positivity loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqheGiZf-9Zi"
      },
      "outputs": [],
      "source": [
        "def l1_regularization(model, device=device) -> torch.Tensor:\n",
        "    l1_reg = torch.tensor(0., requires_grad=True)\n",
        "    for n, p in model.named_parameters():\n",
        "        if re.search(r'weight', n):\n",
        "            l1_reg = l1_reg + torch.norm(p, 1)\n",
        "    return l1_reg.to(device)\n",
        "\n",
        "def pos_penalty(model) -> torch.Tensor:\n",
        "  W = model.fc.weight\n",
        "  # YOUR CODE HERE:\n",
        "  #return ... # positivity constraint to enforce non-negative values in embedding matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF9lmOgdCyGC"
      },
      "source": [
        "Now, we code our main loss function, `trinomial_loss`.\n",
        "\n",
        "It's based on the [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy) loss:\n",
        "\n",
        "$$\n",
        "H(p, q) = -\\sum_{x \\in \\mathcal{X}}p(x)\\log{q(x)}\n",
        "$$\n",
        "\n",
        "Where $p$ and $q$ are two probability distributions, corresponding to the ground-truth distribution and the model's outputs.\n",
        "\n",
        "Basically, it makes sure the similarities between the triplets correspond to the choices in the odd-one-out task.\n",
        "\n",
        "First, we need to compute the similarities between the embeddings (using the dot product).\n",
        "\n",
        "Then, since in the dataset the chosen pair (that doesn't include the odd-one-out) is always the first in the triplet, we just need to ensure that the model's estimated choice probability/similarity for the first pair is always the highest.\n",
        "\n",
        "**EXERCISE:** implement the dot product computation for the three pairs: `pos_sim` (anchor, positive), `neg_sim` (anchor, negative) and `neg_sim_2` (positive, negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x254k-rZCxN4"
      },
      "outputs": [],
      "source": [
        "def compute_similarities(anchor:torch.Tensor, positive:torch.Tensor, negative:torch.Tensor) -> Tuple:\n",
        "    # YOUR CODE HERE\n",
        "    # pos_sim = ...\n",
        "    # neg_sim = ...\n",
        "    # neg_sim_2 = ...\n",
        "\n",
        "    return pos_sim, neg_sim, neg_sim_2\n",
        "\n",
        "def weighted_softmax(sims: tuple, t:float) -> torch.Tensor:\n",
        "  return torch.exp(sims[0] / t) / torch.sum(torch.stack([torch.exp(sim / t) for sim in sims]), dim=0)\n",
        "\n",
        "def cross_entropy_loss(sims:tuple, t:float) -> torch.Tensor:\n",
        "    return torch.mean(-torch.log(weighted_softmax(sims, t)))\n",
        "\n",
        "def trinomial_loss(anchor:torch.Tensor, positive:torch.Tensor, negative:torch.Tensor, t:float) -> torch.Tensor:\n",
        "  sims = compute_similarities(anchor, positive, negative)\n",
        "  return cross_entropy_loss(sims, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEDOVpe9IRKL"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Now we're all set! We just need to set a few hyperparameters:\n",
        "\n",
        "- `lmbda`: this is the weight of the l1-regularization (we can't use the name `lambda` as it's a Python keyword 😁)\n",
        "- `temperature`: the temperature for the softmax function.\n",
        "- `lr`: the learning rate.\n",
        "- `batch_size`: the batch size for training.\n",
        "- `embed_dim`: the embedding's dimensionality.\n",
        "- `n_epochs`: number of training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPfkf1XKGdvL"
      },
      "outputs": [],
      "source": [
        "lmbda = 0.02\n",
        "temperature = 1.\n",
        "lr = 0.001\n",
        "batch_size = 100\n",
        "embed_dim = 40\n",
        "n_epochs = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8hfa7dlL6eQ"
      },
      "source": [
        "Let's define our model, datasets, dataloaders and the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqGppyXEL5jB"
      },
      "outputs": [],
      "source": [
        "model = SPoSE(in_size=n_items, out_size=embed_dim, init_weights=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# Training/validation datasets and dataloaders:\n",
        "trainset = TripletDataset(n_items, train_triplets)\n",
        "validationset = TripletDataset(n_items, val_triplets)\n",
        "trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n",
        "valloader = DataLoader(dataset=validationset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Optimizer (Adam with default settings)\n",
        "optim = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zeXleu0WeJF"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "Ok, now it's time to train the model! With a GPU backend, it should take around 10 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn_9SEg4Mi9L"
      },
      "outputs": [],
      "source": [
        "crossentropies = []\n",
        "complexity_losses = []\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  # To keep track of the losses\n",
        "  batch_crossentropies = torch.zeros(len(trainloader))\n",
        "  batch_complosses = torch.zeros(len(trainloader))\n",
        "  batch_losses_train = torch.zeros(len(trainloader))\n",
        "  batch_accs_train = torch.zeros(len(trainloader))\n",
        "\n",
        "  for i, batch in enumerate(trainloader):\n",
        "    optim.zero_grad()\n",
        "    batch = batch.to(device)\n",
        "    logits = model(batch)\n",
        "\n",
        "    # separate the three embeddings:\n",
        "    anchor, positive, negative = torch.unbind(logits, dim=1)\n",
        "\n",
        "    c_entropy = trinomial_loss(anchor, positive, negative, temperature)\n",
        "    l1_pen = (lmbda/n_items) * l1_regularization(model, device=device)\n",
        "    pos_pen = pos_penalty(model)\n",
        "\n",
        "    # Sum everything into one big loss:\n",
        "    loss = c_entropy + 0.01 * pos_pen + l1_pen\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    batch_losses_train[i] += loss.item()\n",
        "    batch_crossentropies[i] += c_entropy.item()\n",
        "    batch_complosses[i] += l1_pen.item()\n",
        "    batch_accs_train[i] += choice_accuracy(anchor, positive, negative)\n",
        "\n",
        "  avg_crossentropy = torch.mean(batch_crossentropies).item()\n",
        "  avg_comploss = torch.mean(batch_complosses).item()\n",
        "  avg_train_loss = torch.mean(batch_losses_train).item()\n",
        "  avg_train_acc = torch.mean(batch_accs_train).item()\n",
        "\n",
        "\n",
        "  ####################################\n",
        "  # Validation\n",
        "  ####################################\n",
        "\n",
        "  val_accs = torch.zeros(len(valloader))\n",
        "  val_losses = torch.zeros(len(valloader))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(valloader):\n",
        "\n",
        "      batch = batch.to(device)\n",
        "      logits = model(batch)\n",
        "      anchor, positive, negative = torch.unbind(logits, dim=1)\n",
        "\n",
        "      val_loss = trinomial_loss(anchor, positive, negative, temperature)\n",
        "      val_acc = choice_accuracy(anchor, positive, negative)\n",
        "\n",
        "      val_losses[i] += val_loss.item()\n",
        "      val_accs[i] += val_acc.item()\n",
        "\n",
        "  avg_val_loss = torch.mean(val_losses).item()\n",
        "  avg_val_acc = torch.mean(val_accs).item()\n",
        "\n",
        "\n",
        "  print('\\n==========================================================')\n",
        "  print(f'Epoch: {epoch+1}, Train acc: {avg_train_acc:.5f}, Train loss: {avg_train_loss:.5f}, Val acc: {avg_val_acc:.5f}, Val loss: {avg_val_loss:.5f}')\n",
        "  print('==========================================================\\n')\n",
        "\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    # Save the weights every other epoch:\n",
        "    W = model.fc.weight.detach().cpu().numpy().T\n",
        "    np.savetxt(os.path.join(log_dir, f'weights_epoch{epoch+1:04d}.txt'), W)\n",
        "\n",
        "# Save final model\n",
        "W = model.fc.weight.detach().cpu().numpy().T\n",
        "np.savetxt(os.path.join(log_dir, 'weights_final.txt'), W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjlOqywHifwb"
      },
      "source": [
        "If all went well, you should have reached ~64% accuracy on the validation set! That's quite good, given that we are using less than 1/8 of the original dataset for training! Now, let's see what the embedding dimensions we have learned look like. Are they interpretable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XKNEriiiNTq"
      },
      "source": [
        "## Inspect the learned dimensions\n",
        "\n",
        "Let's code some visualization utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VgEyWM-N2sRH"
      },
      "outputs": [],
      "source": [
        "# @title Visualization utilities\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.image as mpimg\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from scipy.io import loadmat\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def plot_weights_across_time(log_dir=log_dir, n_epochs=n_epochs, n_rows=100):\n",
        "\n",
        "    all_epochs = np.arange(2, n_epochs+2, 2)\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    gs = gridspec.GridSpec(1, len(all_epochs)+1, width_ratios=[1]*len(all_epochs) + [0.05])\n",
        "\n",
        "    allweights = []\n",
        "    for epoch in all_epochs:\n",
        "        thisfile = os.path.join(log_dir, f'weights_epoch{epoch:04d}.txt')\n",
        "        allweights.append(filter_nonneg(np.loadtxt(thisfile)[:n_rows]))\n",
        "    allweights = np.dstack(allweights)\n",
        "\n",
        "    vmin = allweights.min()\n",
        "    vmax = allweights.max()\n",
        "\n",
        "    axes = []\n",
        "    for i, epoch in enumerate(all_epochs):\n",
        "        ax = fig.add_subplot(gs[0, i])\n",
        "        cax = ax.matshow(allweights[:,:,i], cmap='viridis', vmin=vmin, vmax=vmax)\n",
        "        ax.set_title(f'Epoch {epoch}')\n",
        "\n",
        "        # Remove ticks and labels\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.tick_params(labelbottom=False, labelleft=False)\n",
        "\n",
        "        axes.append(ax)\n",
        "\n",
        "    # Add a colorbar to the right of the last subplot\n",
        "    cbar_ax = fig.add_subplot(gs[0, -1])\n",
        "    fig.colorbar(cax, cax=cbar_ax)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_top_k(W, dim, k):\n",
        "\n",
        "  sorted_indices = np.argsort(W[:, dim])\n",
        "  topk_indices = list(sorted_indices[-k:][::-1])\n",
        "\n",
        "  return topk_indices\n",
        "\n",
        "def show_top_concepts(W, dim, k, concept_list, img_dir):\n",
        "\n",
        "  fig, axes = plt.subplots(1, k, figsize=(k*5,5))\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  topk_indices = get_top_k(W, dim, k)\n",
        "  for ax, i in zip(axes, topk_indices):\n",
        "    this_concept = concept_list[i]\n",
        "    concept_dir = glob(os.path.join(img_dir, this_concept+'*'))\n",
        "    this_img = random.choice(glob(os.path.join(img_dir, this_concept, '*.jpg')))\n",
        "    this_img = mpimg.imread(this_img)\n",
        "    ax.imshow(this_img)\n",
        "    ax.set_title(this_concept.replace('_', ' '), fontsize=22)\n",
        "    ax.axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def check_rdm_size(rdmA, rdmB):\n",
        "\n",
        "  assert rdmA.shape == rdmB.shape, 'RDMs must have the same size!'\n",
        "  assert rdmA.shape[0] == rdmA.shape[1], 'RDMs must be square!'\n",
        "\n",
        "def plot_rdms(realrdm, modelrdm):\n",
        "\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "  # Plot human RDM\n",
        "  im1 = axs[0].imshow(realrdm, cmap='viridis')\n",
        "  divider1 = make_axes_locatable(axs[0])\n",
        "  cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "  fig.colorbar(im1, cax=cax1)\n",
        "  axs[0].set_title('Human RDM', fontsize=22, pad=15)\n",
        "\n",
        "  # Plot model RDM\n",
        "  im2 = axs[1].imshow(modelrdm, cmap='viridis')\n",
        "  divider2 = make_axes_locatable(axs[1])\n",
        "  cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "  fig.colorbar(im2, cax=cax2)\n",
        "  axs[1].set_title('Model RDM', fontsize=22, pad=15)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def compute_rdm_correlation(rdmA, rdmB):\n",
        "\n",
        "  check_rdm_size(rdmA, rdmB)\n",
        "\n",
        "  loweridx = np.tril_indices(rdmA.shape[0], k=-1)\n",
        "\n",
        "  rdmA = rdmA[loweridx]\n",
        "  rdmB = rdmB[loweridx]\n",
        "\n",
        "  return pearsonr(rdmA, rdmB)[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IxAwEHglX0e"
      },
      "source": [
        "## Plot weights across time\n",
        "\n",
        "We plot, across training epochs, what the weights of our model look like. Can you notice some structure emerging?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1IqHolvh6Lr"
      },
      "outputs": [],
      "source": [
        "plot_weights_across_time(n_epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hspmTrdll476"
      },
      "source": [
        "## Show dimensions\n",
        "\n",
        "For any of our 40 model dimensions, we want to see examples of concepts that maximally activate them. Do the images have anything in common? You can try to make sense of what the different dimensions correspond to, and perhaps name them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm4mhQEGkVpj"
      },
      "outputs": [],
      "source": [
        "# Uncomment this if you need to load the model's final weights again\n",
        "# (e.g. the runtime got disconnected)\n",
        "#W = np.loadtxt(os.path.join(log_dir, 'weights_final.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt7xfWo7mkuc"
      },
      "outputs": [],
      "source": [
        "# Get list of concept names and image directory path\n",
        "\n",
        "things_concepts = pd.read_csv(os.path.join(data_dir, 'things_concepts.tsv'), sep='\\t')\n",
        "concept_list = list(things_concepts['uniqueID'].values)\n",
        "img_dir = os.path.join(data_dir, 'images')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqRbftd3l3Mo"
      },
      "source": [
        "Pick a few dimensions and plot some example concepts/images from each:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy2gqNjCnCND"
      },
      "outputs": [],
      "source": [
        "dim = 0 # @param {type:\"slider\", min:0, max:39, step:1}\n",
        "show_top_concepts(W, dim=dim, k=5, concept_list=concept_list, img_dir=img_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUE8gc_Ulnkc"
      },
      "outputs": [],
      "source": [
        "dim = 29 # @param {type:\"slider\", min:0, max:39, step:1}\n",
        "show_top_concepts(W, dim=dim, k=5, concept_list=concept_list, img_dir=img_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0Sj-EqFlsdI"
      },
      "outputs": [],
      "source": [
        "dim = 18 # @param {type:\"slider\", min:0, max:39, step:1}\n",
        "show_top_concepts(W, dim=dim, k=5, concept_list=concept_list, img_dir=img_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrXp9HVQCPgk"
      },
      "source": [
        "## Compare model and human RDM\n",
        "\n",
        "As a final test of the model we have learned, we check how well it can predict an RDM obtained from human behavioral judgments. After all, this is an RSA tutorial...\n",
        "\n",
        "Why is this a non-trivial task? Because in our training data, only a subset of possible concept pairs was \"seen\" by the model. The model, then, needs to reconstruct the full RDM from a sample. Here, we test it on an RDM of 48x48 concepts.\n",
        "\n",
        "**NOTE:** we are actually turning the Representational **DIS**similarity Matrix into a Representational **Similarity** Matrix, to directly compare it with the dot products (similarities) generated by our model. We still call it an RDM just to make it more confusing.\n",
        "\n",
        "**EXERCISE:** from the onehot encodings of the 48 concepts in the RDM, and the weight matrix, compute the embeddings. Then, from the embeddings, compute the matrix of pairwise dot products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caUKWvzjm5Wi"
      },
      "outputs": [],
      "source": [
        "# Load the RDM and turn it into an \"RSM\":\n",
        "rdm48 = loadmat(os.path.join(data_dir, 'RDM48_triplet.mat'))['RDM48_triplet']\n",
        "rdm48 = 1. - rdm48\n",
        "\n",
        "# Load the list of 48 concepts used for this RDM, so we can feed them to the model:\n",
        "words48 = [w[0][0] for w in loadmat(os.path.join(data_dir, 'words48.mat'))['words48']]\n",
        "ids48 = []\n",
        "\n",
        "for w in words48:\n",
        "  w = w.replace(' ', '_')\n",
        "  if w not in concept_list:\n",
        "    pattern = re.compile(f\"^{re.escape(w)}\\d+$\")\n",
        "    theseconcepts = [c for c in concept_list if pattern.match(c)]\n",
        "    w = random.choice(theseconcepts)\n",
        "  ids48.append(concept_list.index(w))\n",
        "\n",
        "# Encode them as one-hot vectors:\n",
        "onehot48 = np.eye(n_items)[np.array(ids48)]\n",
        "\n",
        "# Get the embeddings by feeding the one-hot vectors\n",
        "# to the network:\n",
        "# YOUR CODE HERE\n",
        "# embeddings48 = ...\n",
        "\n",
        "# From the 48 x 40 embeddings matrix, get the 40 x 40 matrix\n",
        "# of rowwise dot products:\n",
        "# YOUR CODE HERE\n",
        "# rdm48_model = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZyKaqCp8-6Y"
      },
      "outputs": [],
      "source": [
        "# Plot the two RDMs side by side\n",
        "plot_rdms(rdm48, rdm48_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cbVRtx5p2V1"
      },
      "source": [
        "They look quite similar! This is promising... let's check how correlated they are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8eh-Yxm2ija"
      },
      "outputs": [],
      "source": [
        "compute_rdm_correlation(rdm48, rdm48_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB03nwuPqH5-"
      },
      "source": [
        "In conclusion, we can say that:\n",
        "\n",
        "- SPoSE can learn to accurately predict human choices in a triplet odd-one-out task.\n",
        "\n",
        "- It does so by generating interpretable dimensions underlying human judgments.\n",
        "\n",
        "- It is able to generalize and predict full pairwise similarity matrices from a separate experiment.\n",
        "\n",
        "Really not bad for only having 1854 x 40 linear weights!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
