{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfKMTT7sRKaV"
      },
      "source": [
        "# Day 4: Lesion Analysis to Identify Functional Units in a Neural Network\n",
        "\n",
        "Neural networks can be extremely powerful in solving problems computationally but due to their mathematical complexity they have often been deemed a black box into which we are simply inputting something, the network somehow analyzes it and arrives at a conclusion. However, it has been different to really understand how a network decomposes a task. In the following tutorial we will approach this problem by lesioning some components in the network to see how these lesions affect network performance. Lesioning neural networks is relatively similar to lesioning real brains though much easier and comes with less ethical complications. Instead of hoping for a lesioned patient or performing tDCS or TMS on a healthy patient, we can simply change some numeric values in the network weights. Lesioning becomes particularly useful when we have a network that has to solve multiple tasks at once as it allows us to see whether segregated streams, like those in the brain, are advantageous to processing. Multi-task network offer the exciting possibility to reinstate more similar conditions to the one that the visual system in the brain faced. Rather than evolving very task-specific networks one at a time, the brain had to evolve into a network solving multiple tasks at the same time. The idea is that if we see similarly segregated streams in a neural network compared to the brain, that could mean that this task decomposition is an optimized solution to the tasks our brain is facing.\n",
        "\n",
        "In this tutorial, you will learn:\n",
        "\n",
        "*   how to lesion a kernel in a neural network\n",
        "*   how to analyze task division in a dual-task network\n",
        "*   how to visualize task-specific kernel input preferences\n",
        "\n",
        "\n",
        "Before jumping into the main tutorial we will quickly import all the libraries we need as well as the datasets and pretrained model weights.\n",
        "\n",
        "As a last note: During the tutorial you will see ellipses (...) pop up in the code. These mean that you need to fill in some code yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE-RlUk5RI1F"
      },
      "outputs": [],
      "source": [
        "# These are some basic data science libraries that allow us to investigate our datasets and plot some graphs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# These are all the libraries that will allow us to do Deep Learning\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# TQDM is a useful library that implements progress bars for for-loops\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# These are some file-management libraries we will need in the beginning\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMPgomGdsg8o"
      },
      "source": [
        "Great, all libraries are imported now.\n",
        "\n",
        "Now let's get our dataset and the pretrained model. You can do that by linking the following Google Drive folder to your own Drive using a shortcut. Then you can link your drive to Google Colab and you are good to go.\n",
        "\n",
        "Here is the link to the shared folder:\n",
        "\n",
        "[Click me](https://drive.google.com/drive/folders/1yFNw65xd79dxlgnyrdxI5U2BwyjRVh7h?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzGeaecypYjw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "source_folder = '/content/drive/MyDrive/SummerSchoolTutorials'\n",
        "destination_folder = '/content/SummerSchoolTutorials'\n",
        "\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "\n",
        "# Copy files from Google Drive to the local directory\n",
        "for item in os.listdir(source_folder):\n",
        "    source_path = os.path.join(source_folder, item)\n",
        "    destination_path = os.path.join(destination_folder, item)\n",
        "    if os.path.isfile(source_path):\n",
        "        shutil.copy(source_path, destination_path)\n",
        "\n",
        "# Unzip the zip files into the local directory\n",
        "for item in os.listdir(destination_folder):\n",
        "    if item.endswith('.zip'):\n",
        "        zip_path = os.path.join(destination_folder, item)\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(destination_folder)\n",
        "        os.remove(zip_path)  # Remove the zip file after extracting\n",
        "\n",
        "# Remove __MACOSX directories if they exist\n",
        "for root, dirs, files in os.walk(destination_folder):\n",
        "    if '__MACOSX' in dirs:\n",
        "        shutil.rmtree(os.path.join(root, '__MACOSX'))\n",
        "\n",
        "# List the files in the local directory to confirm\n",
        "os.listdir(destination_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hODOtal4WW3-"
      },
      "source": [
        "## Network Investigation and Kernel Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCF8gkjvpZ-o"
      },
      "source": [
        "Alright, now we have all the files and libraries we need. For this tutorial we will be using a network that has been trained on two tasks (object and face recognition). The network architecture is VGG16, which is the same architecture that was used in the studies that were presented today. While there are multiple ways to train a multi-task network, the simplest one arguably is just to concatenate the datasets and extend the output channels of the last layer of the classifier to include to combined number of classes. This means that the tasks in our network are divided by the input images, rather than each image being applicable to all tasks and using either a multi-label setup or a multi-task head setup.\n",
        "\n",
        "First let's print the model to have an overview of what we are working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pX_VE0rRU1d"
      },
      "outputs": [],
      "source": [
        "dual_vgg = torchvision.models.vgg16(pretrained = False)\n",
        "\n",
        "print(dual_vgg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u_BGcLFwNy-"
      },
      "source": [
        "As you can see, the model has an encoder with 13 convolutional layers and a decoder/classifier with 3 fully-connected layers. We can also see that the final classifier layer still has 1000 output channels. This is because the included models in PyTorch are specialized for the ImageNet-1k dataset which as the name suggests has 1000 classes. Thus, we need to change the output channels of the last layer to properly load our weights.\n",
        "The dual task network we will be using is the exact same network that was used in the studies by Katharina Dobs. However, instead of working with the entire dataset, we will be working only with a very small subset today to save some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkHqZLJ7woEn"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('/content/SummerSchoolTutorials/Weights/epoch_50.pth.tar', map_location = 'cpu')\n",
        "\n",
        "# Extract the state_dict for the model\n",
        "state_dict = checkpoint['state_dict']\n",
        "\n",
        "dual_vgg_weights = {}\n",
        "for k, v in state_dict.items():\n",
        "    if k.startswith('module.'):\n",
        "        dual_vgg_weights[k[7:]] = v\n",
        "    else:\n",
        "        dual_vgg_weights[k] = v\n",
        "\n",
        "num_classes = 2137\n",
        "\n",
        "num_features = dual_vgg.classifier[6].in_features\n",
        "dual_vgg.classifier[6] = nn.Linear(num_features, num_classes)\n",
        "\n",
        "dual_vgg.load_state_dict(dual_vgg_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiwNi6qlq6oo"
      },
      "source": [
        "Great, now the model is ready to go!\n",
        "\n",
        "One of the easiest ways to investigate the features a neural network extracts is to simply pass an image into it and record the activation maps. The following code creates a function that does so and overlays the activation map from a kernel of your choice over the original image.\n",
        "\n",
        "You can choose an input image of your choice and insert the path into the function. Ideally, choose one image of a face and one image of an object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2R742f9si4d"
      },
      "outputs": [],
      "source": [
        "def visualize_image_activations(image_path, network, layer_index, kernel_index):\n",
        "    # Preprocess the image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    img_t = transform(Image.open(image_path)).unsqueeze(0)\n",
        "\n",
        "    # Register hook to extract feature maps\n",
        "    def extract_maps(_, __, output):\n",
        "        global feature_maps\n",
        "        feature_maps = output\n",
        "\n",
        "    hook = network.features[layer_index].register_forward_hook(extract_maps)\n",
        "\n",
        "    # Forward pass through the network\n",
        "    with torch.no_grad():\n",
        "        _ = network.features(img_t)\n",
        "    hook.remove()\n",
        "\n",
        "    # Process and visualize the feature map\n",
        "    feature_maps_s = feature_maps.squeeze(0)\n",
        "\n",
        "    plt.imshow(feature_maps_s[kernel_index].numpy(), cmap = 'viridis')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    feature_maps_n = (feature_maps_s - feature_maps_s.min()) / (feature_maps_s.max() - feature_maps_s.min())\n",
        "\n",
        "    feature_maps_u = feature_maps_n.unsqueeze(0)\n",
        "    feature_maps_r = torch.nn.functional.interpolate(feature_maps_u, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "    feature_maps_s2 = feature_maps_r.squeeze(0)\n",
        "\n",
        "    feature_map_num = feature_maps_s2[kernel_index].detach().numpy()\n",
        "    feature_map_colored = plt.cm.jet(feature_map_num)[:, :, :3]\n",
        "\n",
        "    input_image_np = img_t.squeeze(0).permute(1, 2, 0).numpy()\n",
        "    input_image_np = (input_image_np - input_image_np.min()) / (input_image_np.max() - input_image_np.min())\n",
        "\n",
        "    overlay = input_image_np * 0.3 + feature_map_colored * 0.7  # Blend the images\n",
        "\n",
        "    plt.imshow(overlay)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "visualize_image_activations(image_path = '/path/to/your/image.jpg', network = dual_vgg, layer_index = ..., kernel_index = ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Fi1CK2V1bV"
      },
      "source": [
        "**ToDo (1)**:\n",
        "\n",
        "Run this to compare a couple activation maps from different layers to get an approximate idea of the models processing steps. If you are struggling, you can reveal some code below by clicking \"Show code\". Remember, you still need to input the path to your image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLjrVR2vV6_g"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "...\n",
        "\n",
        "...\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "image = '/path/to/your/image.jpg'\n",
        "\n",
        "print('Layer 0')\n",
        "visualize_image_activations(image_path = image, network = dual_vgg, layer_index = 0, kernel_index = 20)\n",
        "print('')\n",
        "\n",
        "print('Layer 10')\n",
        "visualize_image_activations(image_path = image, network = dual_vgg, layer_index = 10, kernel_index = 20)\n",
        "print('')\n",
        "\n",
        "print('Layer 19')\n",
        "visualize_image_activations(image_path = image, network = dual_vgg, layer_index = 19, kernel_index = 20)\n",
        "print('')\n",
        "\n",
        "print('Layer 28')\n",
        "visualize_image_activations(image_path = image, network = dual_vgg, layer_index = 28, kernel_index = 20)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DRjyTsVDTJvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdPVKcpM-9A-"
      },
      "source": [
        "You quickly notice that this method is quite ineffective in identifying how to network is structured and dividied across task. This is where the lesions come in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0afXNdneLIf0"
      },
      "source": [
        "**ToThink (1)**:\n",
        "\n",
        "What property of the neural network allows the deeper neurons to be sensitive to more complex patterns? What is the analogue of that property in the visual system in the brain? You can write your answer below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq81XU48LVRA"
      },
      "source": [
        "*Write your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1c3vcmYvW1A"
      },
      "source": [
        "## Lesioning Kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi_FvrH9-fT6"
      },
      "source": [
        "Kernels in a neural network are essentially weight matrices that are used for convolving over an input matrix. We can access and visualize these kernels by indexing into the network. The purpose of a lesion is to disable a part of a network so that its feature extractions are not able to be used by later layers. In the brain, this can be done through several ways, some more invasive than other but they usually either significantly lower the frequency of firing (decreasing the information transmitted) of the neuron/kernel or  make it completely inactive (removing any transmitted information). This process is much easier in neural networks and can be particularly useful in identifying how that part of the network influences its behavior.\n",
        "\n",
        "Let's take the network we have just investigated and look at a kernel of our choice. We will also save original weights as we want to avoid having to reload the weights every time after lesioning. To have a good comparison we will visualize the results of that method as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrOty7MKUHvP"
      },
      "outputs": [],
      "source": [
        "layer = ...\n",
        "kernel = ...\n",
        "\n",
        "image = '/path/to/your/image.jpg'\n",
        "\n",
        "orig_weights = dual_vgg.features[layer].weight.data[kernel].clone()\n",
        "orig_bias = dual_vgg.features[layer].bias.data[kernel].clone()\n",
        "\n",
        "visualize_image_activations(image_path = image, network = dual_vgg, layer_index = layer, kernel_index = kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3R2GCRLVYUQ"
      },
      "source": [
        "Now that we know how that kernel looks without a lesion, let us do the lesion to confirm that the kernel has been ablated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHyj14NdVfxl"
      },
      "outputs": [],
      "source": [
        "dual_vgg.features[layer].weight.data[kernel] = 0.0\n",
        "dual_vgg.features[layer].bias.data[kernel] = 0.0\n",
        "\n",
        "visualize_image_activations(image_path = image, network = dual_vgg, layer_index = layer, kernel_index = kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elzLEkxXhuae"
      },
      "source": [
        "As you can see, while the feature map showed activation before the lesion, there now is no activation remaining afterwards. This means that the lesion was successful. But what is the impact of that lesion considering that the next layer after this one is now missing a sensible feature map to convolve over, which will likely have some effects on overall network behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD9BSifHivfb"
      },
      "outputs": [],
      "source": [
        "# Before going to the next section, let's reset the kernel quickly\n",
        "\n",
        "dual_vgg.features[layer].weight.data[kernel] = orig_weights\n",
        "dual_vgg.features[layer].bias.data[kernel] = orig_bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yObhEkUGWnNh"
      },
      "source": [
        "## Relating Lesions to Network Behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNs_Mmo_iInr"
      },
      "source": [
        "To investigate how a lesion affects the network behavior we can simply compare the loss before the lesion with after the lesion similarly to how we would compare human behavior before a temporary lesion with after the lesion. This measure will give us an estimate of how important this particular feature map is to the task that the loss was computed on.\n",
        "\n",
        "Let's start by preparing the dataset:\n",
        "\n",
        "To properly do the analysis we need to create a filter for the entire dataset so that we can selectively have batches with images from only one task. Because our dataset is already very small (20 instances per class) we will not be doing a train/val split for this analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvjX7yDmX3ol"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/SummerSchoolTutorials/Datasets/DualDataset'\n",
        "\n",
        "transform = transforms.Compose([ transforms.Resize((256, 256)),\n",
        "                                 transforms.CenterCrop(224),\n",
        "                                 transforms.ToTensor(),\n",
        "                                 transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "# Load the dataset\n",
        "dataset = ImageFolder(root=dataset_path, transform=transform)\n",
        "\n",
        "# Custom dataset to filter by prefix: 'face' if face class, 'object' if object class\n",
        "class FilteredDataset(Dataset):\n",
        "    def __init__(self, dataset, prefix):\n",
        "        self.dataset = dataset\n",
        "        self.indices = [i for i, (path, _) in enumerate(dataset.samples) if os.path.basename(os.path.dirname(path)).startswith(prefix)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_idx = self.indices[idx]\n",
        "        return self.dataset[original_idx]\n",
        "\n",
        "# Create datasets for faces and objects\n",
        "face_dataset = FilteredDataset(dataset, 'face')\n",
        "object_dataset = FilteredDataset(dataset, 'object')\n",
        "\n",
        "# Create data loaders\n",
        "face_dataloader = DataLoader(face_dataset, batch_size=32, shuffle=True)\n",
        "object_dataloader = DataLoader(object_dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHlxBwMkYeN1"
      },
      "source": [
        "Now that the dataloaders are ready, we can continue by probing how the lesion of a single kernel affects the network behavior. To do so we will first establish a baseline measure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVnEYq-9ZSdk"
      },
      "outputs": [],
      "source": [
        "dual_vgg = dual_vgg.to(device)\n",
        "\n",
        "dual_vgg.eval()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Get base values\n",
        "pbar1 = tqdm(enumerate(face_dataloader), desc = 'Establishing Face Loss Baseline', total = len(face_dataloader))\n",
        "pbar2 = tqdm(enumerate(object_dataloader), desc = 'Establishing Object Loss Baseline', total = len(object_dataloader))\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    batch_losses = np.zeros(len(face_dataloader), dtype = float)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for idx, batch in pbar1:\n",
        "\n",
        "        image, label = batch\n",
        "\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        output = dual_vgg(image)\n",
        "\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        predicted = torch.argmax(output, 1)\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == label).sum().item()\n",
        "\n",
        "        batch_losses[idx] = loss.item()\n",
        "\n",
        "    base_face_loss = np.mean(batch_losses)\n",
        "    base_face_loss_sd = np.std(batch_losses)\n",
        "\n",
        "    print(f'Face Loss: {base_face_loss}, Face Loss SD {base_face_loss_sd}')\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    batch_losses = np.zeros(len(object_dataloader), dtype = float)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for idx, batch in pbar2:\n",
        "\n",
        "        image, label = batch\n",
        "\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        output = dual_vgg(image)\n",
        "\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        predicted = torch.argmax(output, 1)\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == label).sum().item()\n",
        "\n",
        "        batch_losses[idx] = loss.item()\n",
        "\n",
        "    base_object_loss = np.mean(batch_losses)\n",
        "    base_object_loss_sd = np.std(batch_losses)\n",
        "\n",
        "    print(f'Object Loss: {base_object_loss}, Object Loss SD {base_object_loss_sd}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this takes too long, you can also load the following cell that can be revealed by clicking \"Show code\". It contains the loss values and standard deviations from my own runs."
      ],
      "metadata": {
        "id": "q0gJtrZCIiiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "base_face_loss = 1.4565794246632662\n",
        "base_face_loss_sd = 0.4835781355369047\n",
        "\n",
        "base_object_loss = 2.281959489156615\n",
        "base_object_loss_sd = 0.45413462432476875"
      ],
      "metadata": {
        "cellView": "form",
        "id": "awBkXNKGIh4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYuWNqdhcBv9"
      },
      "source": [
        "Great, now that we have some values to compare to, lets lesion a kernel and repeat this analysis. To help with the process, I have defined a function below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnLOHGQucMyM"
      },
      "outputs": [],
      "source": [
        "def test_lesion_effect(task, dataloader, network, layer_idx, kernel_idx, batches = 100):\n",
        "\n",
        "    network = network.to(device)\n",
        "\n",
        "    pbar = tqdm(enumerate(dataloader), desc=f'Establishing {task} Loss Effect for Kernel {kernel_idx}', total=batches)\n",
        "\n",
        "    # Save original kernel weights\n",
        "\n",
        "    orig_weights = network.features[layer_idx].weight.data[kernel_idx].clone()\n",
        "    if network.features[layer_idx].bias is not None:\n",
        "        orig_bias = network.features[layer_idx].bias.data[kernel_idx].clone()\n",
        "    else:\n",
        "        orig_bias = None\n",
        "\n",
        "    # Lesion kernel\n",
        "\n",
        "    network.features[layer_idx].weight.data[kernel_idx] = 0.0\n",
        "    if network.features[layer_idx].bias is not None:\n",
        "        network.features[layer_idx].bias.data[kernel_idx] = 0.0\n",
        "\n",
        "    network.eval()\n",
        "\n",
        "    # Measure Effect on Loss\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for idx, batch in pbar:\n",
        "\n",
        "            if idx >= batches:\n",
        "                break\n",
        "\n",
        "            image, label = batch\n",
        "\n",
        "            image, label = image.to(device), label.to(device)\n",
        "\n",
        "            output = network(image)\n",
        "\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            pbar.set_postfix({'loss': running_loss / (idx + 1)})\n",
        "\n",
        "        lesion_loss = running_loss/batches\n",
        "\n",
        "        network.features[layer_idx].weight.data[kernel_idx] = orig_weights\n",
        "        if orig_bias is not None:\n",
        "            network.features[layer_idx].bias.data[kernel_idx] = orig_bias\n",
        "\n",
        "    return lesion_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTusRJH4f8NE"
      },
      "source": [
        "Okay, now we know what the absolute change in loss is for each task when lesioning a single kernel, but this is not super informative yet. Because they are different tasks, the losses may operate on different scales especially as the network may not learn both tasks equally well. To fix this, we must establish a relative measure within tasks to identify which kernels are important for each task.\n",
        "\n",
        "**ToDo (2)**:\n",
        "\n",
        "Iterate through some of the kernels in a deeper layer and save the results to a list containing the kernel index and the associated effect on the loss. An entire layer of 512 kernels would take quite long even on GPU, so we will only do a smaller version of this process by running the analysis for only 8 kernels instead so that you have seen it running once. When writing your code, make sure to print the difference between the lesion loss and the original loss of the task. This will help you see better how a lesion affects each task. Then, visualize the entire results using a bar graph with one bar for each task-kernel combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG26J0oD1XoS"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "...\n",
        "\n",
        "...\n",
        "\n",
        "...\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can reveal the correct code below by clicking \"Show code\"."
      ],
      "metadata": {
        "id": "f5jXhR8EApmI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbgAR-BJ5rn2"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "dual_vgg.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "ranking = np.zeros((8, 2), dtype = float)\n",
        "\n",
        "for kidx in range(8):\n",
        "    raw_effect_face = test_lesion_effect('Face', face_dataloader,  dual_vgg, 28, kidx, batches = 50)\n",
        "\n",
        "    face_effect = raw_effect_face - base_face_loss\n",
        "\n",
        "    raw_effect_object = test_lesion_effect('Object', object_dataloader, dual_vgg, 28, kidx, batches = 50)\n",
        "\n",
        "    object_effect = raw_effect_object - base_object_loss\n",
        "\n",
        "    print(f'Kernel {kidx}: Effect on Face Loss: {face_effect}')\n",
        "    print(f'Kernel {kidx}: Effect on Object Loss: {object_effect}')\n",
        "\n",
        "    ranking[kidx, 0] = face_effect\n",
        "    ranking[kidx, 1] = object_effect\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "index = np.arange(8)\n",
        "bar_width = 0.35\n",
        "\n",
        "# Plotting bars for face and object effects\n",
        "face_bars = ax.bar(index, ranking[:, 0], bar_width, label='Face Effect')\n",
        "object_bars = ax.bar(index + bar_width, ranking[:, 1], bar_width, label='Object Effect')\n",
        "\n",
        "ax.set_xlabel('Kernel')\n",
        "ax.set_ylabel('Effect')\n",
        "ax.set_title('Face and Object Effects on Task Loss')\n",
        "ax.set_xticks(index + bar_width / 2)\n",
        "ax.set_xticklabels(index)\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also directly load the results for the first 8 kernels in the layer by loading the cell below:"
      ],
      "metadata": {
        "id": "31Q-e2dSAixW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "ranking = np.array([[-0.24118632, -0.15766363],\n",
        "                    [-0.1633949,   0.05389339],\n",
        "                    [ 0.06806312,  0.09304704],\n",
        "                    [ 0.02567353, -0.10276617],\n",
        "                    [-0.16002726, -0.03638811],\n",
        "                    [ 0.04657509, -0.03296645],\n",
        "                    [-0.07809152,  0.07486088],\n",
        "                    [-0.03144749, -0.01088203]])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "index = np.arange(8)\n",
        "bar_width = 0.35\n",
        "\n",
        "# Plotting bars for face and object effects\n",
        "face_bars = ax.bar(index, ranking[:, 0], bar_width, label='Face Effect')\n",
        "object_bars = ax.bar(index + bar_width, ranking[:, 1], bar_width, label='Object Effect')\n",
        "\n",
        "ax.set_xlabel('Kernel')\n",
        "ax.set_ylabel('Effect')\n",
        "ax.set_title('Face and Object Effect on Task Loss')\n",
        "ax.set_xticks(index + bar_width / 2)\n",
        "ax.set_xticklabels(index)\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qNmHP4TU_OMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While you can see that there are some kernels that seem to similarly contribute to both tasks, quite a few seem to show high preference for one task, increasing that task's loss by much more than the other task's loss (sometimes even improving performance for the other task by lowering loss)."
      ],
      "metadata": {
        "id": "NwtvU0X-L-Qz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQPWsG495vIT"
      },
      "source": [
        "**ToThink (2)**:\n",
        "\n",
        "Which lesion effects are we measuring with this method and which are we missing out on? Describe a method that can overcome this limitation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCftQ5HD6Anl"
      },
      "source": [
        "*Write your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kAStGRDX3yM"
      },
      "source": [
        "### Visualizing and Optimizing Task-Specific Kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vei3tnJ2YBYR"
      },
      "source": [
        "Now that we have a list of kernels and their associated loss values for each task, we can create a ranking to identify the most important kernels for either task. Because the previous ranking was only on a subset of kernels and we do not want you to have to wait multiple hours, we will provide you with a full ranking of the previous task. Using this ranking, we can identify the most useful kernels for each task. In order to understand their functioning, we will use Gradient Ascent on a randomized input to maximize the mean activation of the feature maps resulting from the kernel. This means that in each iteration the algorithm slightly changes the input image in order to maximize the activation leading to the visualization of the preferred input of a kernel.\n",
        "\n",
        "**ToDo (3)**:\n",
        "\n",
        "Use the full ranking to find one object-preferring kernel and one face-preferring kernel and visualize their preferred input images using Gradient Ascent on a randomized input. Part of the code is already prepared so your task is to fill in the blanks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obj_array = np.load('/content/SummerSchoolTutorials/selections_record_dual_obj.npz', allow_pickle=True)\n",
        "object_ranking = obj_array['selected_units']\n",
        "\n",
        "print(f'Top10 Object Kernels: {object_ranking[:10]}')\n",
        "\n",
        "face_array = np.load('/content/SummerSchoolTutorials/selections_record_dual_face.npz', allow_pickle=True)\n",
        "face_ranking = face_array['selected_units']\n",
        "\n",
        "print(f'Top10 Face Kernels: {face_ranking[:10]}')"
      ],
      "metadata": {
        "id": "1qSbAb-HhvBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, now we know the top face kernels and the top object kernels. Let's find out what features they are sensitive to by doing Gradient Ascent."
      ],
      "metadata": {
        "id": "67fVa_veGixj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUldlO1JYkuD"
      },
      "outputs": [],
      "source": [
        "# After finding the top-ranked kernels, continue here\n",
        "\n",
        "def optimize_input(network, layer_idx, kernel_idx):\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  input_image = torch.randn(1, 3, 224, 224, requires_grad=True, device=device)\n",
        "\n",
        "  network = network.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam([input_image], lr = 0.1)\n",
        "\n",
        "  layer = network[layer_idx]\n",
        "\n",
        "  def extract_activation(module, input, output):\n",
        "\n",
        "      global activation\n",
        "\n",
        "      activation = output[:, kernel_idx, :, :].clone()\n",
        "\n",
        "  hook = layer.register_forward_hook(extract_activation)\n",
        "\n",
        "  iterations = 250\n",
        "\n",
        "  pbar = tqdm(range(iterations), desc = 'Image Optimization', total = iterations)\n",
        "\n",
        "  for i in pbar:\n",
        "\n",
        "    ...\n",
        "\n",
        "    ...\n",
        "\n",
        "    loss = ...\n",
        "\n",
        "    ...\n",
        "\n",
        "    ...\n",
        "\n",
        "    pbar.set_postfix({'Loss': loss.item()})\n",
        "\n",
        "\n",
        "  def rescale(img):\n",
        "    img = img - img.min()\n",
        "    img = img / img.max()\n",
        "\n",
        "    return img\n",
        "\n",
        "  img = input_image.detach().cpu().numpy()[0]\n",
        "  img = np.transpose(rescale(img), (1, 2, 0))\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "  hook.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find the full function code below by clicking \"Show code\"."
      ],
      "metadata": {
        "id": "dccjVtCEEXz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def optimize_input(network, layer_idx, kernel_idx):\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  input_image = torch.randn(1, 3, 224, 224, requires_grad=True, device=device)\n",
        "\n",
        "  network = network.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam([input_image], lr = 0.1)\n",
        "\n",
        "  layer = network.features[layer_idx]\n",
        "\n",
        "  def extract_activation(module, input, output):\n",
        "\n",
        "      global activation\n",
        "\n",
        "      activation = output[:, kernel_idx, :, :].clone()\n",
        "\n",
        "  hook = layer.register_forward_hook(extract_activation)\n",
        "\n",
        "  iterations = 250\n",
        "\n",
        "  pbar = tqdm(range(iterations), desc = 'Image Optimization', total = iterations)\n",
        "\n",
        "  for i in pbar:\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    network(input_image)\n",
        "\n",
        "    loss = - activation.mean()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    pbar.set_postfix({'Loss': loss.item()})\n",
        "\n",
        "\n",
        "  def rescale(img):\n",
        "    img = img - img.min()\n",
        "    img = img / img.max()\n",
        "\n",
        "    return img\n",
        "\n",
        "  img = input_image.detach().cpu().numpy()[0]\n",
        "  img = np.transpose(rescale(img), (1, 2, 0))\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "  hook.remove()"
      ],
      "metadata": {
        "id": "jkxlbx1UEdAr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the completed function here:"
      ],
      "metadata": {
        "id": "XBROZ1mdE1n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = ...\n",
        "kernel = ...\n",
        "\n",
        "optimize_input(dual_vgg, layer, kernel)"
      ],
      "metadata": {
        "id": "crHRq_SOEzmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnrUIQ_KXOxA"
      },
      "source": [
        "## Large-Scale Lesions and Task Segregation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgPQ2roXWuIK"
      },
      "source": [
        "Great, we now know which kernels are useful for either task and have a rough idea of what they are responsive to. The next step is to develop a measure that characterizes how divided a layer is between tasks. Are there many kernels that contribute similarly to both tasks or are there many specialized kernels?\n",
        "\n",
        "To do this, we are gonna use the ranking of the Top50 kernels in the last convolutional layer for both tasks that Katharina Dobs has provided us for this tutorial, and do a larger-scale lesion of task-specific kernels to see how they affect our network behavior. Then we pass the data set through and measure how these large scale lesions affect the loss values of the two tasks. To evaluate the changes in loss following the lesions, we will use normalized values.\n",
        "\n",
        "We will use these kernels for our larger lesion. The next step is to do the lesion and determine the normalized loss change. Remember that we already computed the baseline losses and standard deviations before when we did the single kernel lesions. We can use these values now in the computation of normalized loss changes. Usually, we would have to do this again as you should use a different dataset for this part of the evaluation but for simplicity and storage reasons, we will use this dataset again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLfMhhniWuwz"
      },
      "outputs": [],
      "source": [
        "print(f'Mean of Object Loss: {base_object_loss}')\n",
        "print(f'Mean of Face Loss: {base_face_loss}')\n",
        "\n",
        "print(f'Spread of Object Loss: {base_object_loss_sd}')\n",
        "print(f'Spread of Face Loss: {base_face_loss_sd}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOsQVyiPjhaz"
      },
      "source": [
        "Alright, we now have all the measures required for calculating normalized loss changes. The following is a function that computes the entire normalized loss changes for all combinations of lesions and task evaluations. This is a long code of block, so take a few moments to look at the function and what it does."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def largelesiontest(network, face_set, object_set, face_ranking, object_ranking, lesion_extent = 0.2, num_batches = 200):\n",
        "\n",
        "  base_mean_face_loss = 1.4565794246632662\n",
        "  base_spread_face_loss = 0.4835781355369047\n",
        "\n",
        "  base_mean_object_loss = 2.281959489156615\n",
        "  base_spread_object_loss = 0.45413462432476875\n",
        "\n",
        "  num_kernels = 512\n",
        "\n",
        "  # First we determine how many kernels we need to lesion\n",
        "\n",
        "  lesioned_kernel_num = int(lesion_extent * num_kernels)\n",
        "\n",
        "  lesioned_face_kernels = face_ranking[:lesioned_kernel_num]\n",
        "  lesioned_object_kernels = object_ranking[:lesioned_kernel_num]\n",
        "\n",
        "  # We need to save the original weights so that we can reinstate them for later use\n",
        "\n",
        "  orig_weights = network.features[28].weight.data.clone()\n",
        "  orig_biases  = network.features[28].bias.data.clone()\n",
        "\n",
        "  # Now we can lesion the kernels and record the loss for both tasks\n",
        "\n",
        "  for idx in lesioned_face_kernels:\n",
        "    network.features[28].weight.data[idx] = 0.0\n",
        "    network.features[28].bias.data[idx] = 0.0\n",
        "\n",
        "  pbar1 = tqdm(enumerate(face_dataloader), desc = f'Working on Face Dataloader for Face Lesion', total = num_batches)\n",
        "  pbar2 = tqdm(enumerate(object_dataloader), desc = f'Working on Object Dataloader for Face Lesion', total = num_batches)\n",
        "\n",
        "  network.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    batch_losses = np.zeros(num_batches, dtype = float)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for idx, batch in pbar1:\n",
        "\n",
        "        if idx >= num_batches:\n",
        "            break\n",
        "\n",
        "        image, label = batch\n",
        "\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        output = network(image)\n",
        "\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        predicted = torch.argmax(output, 1)\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == label).sum().item()\n",
        "\n",
        "        batch_losses[idx] = loss.item()\n",
        "\n",
        "    lesion_face_loss = np.mean(batch_losses)\n",
        "\n",
        "    # We determine the normalized loss change as the amount of standard deviations the new performance is away from default\n",
        "\n",
        "    f2f_normloss_change = (lesion_face_loss - base_mean_face_loss) / base_spread_face_loss\n",
        "\n",
        "    print(f'For Face Lesion: Normalized Face Loss Change: {f2f_normloss_change}')\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    batch_losses = np.zeros(num_batches, dtype = float)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for idx, batch in pbar2:\n",
        "\n",
        "        if idx >= num_batches:\n",
        "            break\n",
        "\n",
        "        image, label = batch\n",
        "\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        output = network(image)\n",
        "\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        predicted = torch.argmax(output, 1)\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == label).sum().item()\n",
        "\n",
        "        batch_losses[idx] = loss.item()\n",
        "\n",
        "    lesion_object_loss = np.mean(batch_losses)\n",
        "\n",
        "    f2o_normloss_change = (lesion_object_loss - base_mean_object_loss) / base_spread_object_loss\n",
        "\n",
        "    print(f'For Face Lesion: Normalized Object Loss Change: {f2o_normloss_change}')\n",
        "\n",
        "  # Restore Model\n",
        "\n",
        "  network.features[28].weight.data = orig_weights\n",
        "  network.features[28].bias.data = orig_biases\n",
        "\n",
        "  # Repeat the process for the object lesion\n",
        "\n",
        "  for idx in lesioned_object_kernels:\n",
        "    network.features[28].weight.data[idx] = 0.0\n",
        "    network.features[28].bias.data[idx] = 0.0\n",
        "\n",
        "  pbar1 = tqdm(enumerate(face_dataloader), desc = f'Working on Face Dataloader for Object Lesion', total = num_batches)\n",
        "  pbar2 = tqdm(enumerate(object_dataloader), desc = f'Working on Object Dataloader for Object Lesion', total = num_batches)\n",
        "\n",
        "  network.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    batch_losses = np.zeros(num_batches, dtype = float)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for idx, batch in pbar1:\n",
        "\n",
        "        if idx >= num_batches:\n",
        "            break\n",
        "\n",
        "        image, label = batch\n",
        "\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        output = network(image)\n",
        "\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        predicted = torch.argmax(output, 1)\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == label).sum().item()\n",
        "\n",
        "        batch_losses[idx] = loss.item()\n",
        "\n",
        "    lesion_face_loss = np.mean(batch_losses)\n",
        "\n",
        "    o2f_normloss_change = (lesion_face_loss - base_mean_face_loss) / base_spread_face_loss\n",
        "\n",
        "    print(f'For Object Lesion: Normalized Face Loss Change: {o2f_normloss_change}')\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    batch_losses = np.zeros(num_batches, dtype = float)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for idx, batch in pbar2:\n",
        "\n",
        "        if idx >= num_batches:\n",
        "            break\n",
        "\n",
        "        image, label = batch\n",
        "\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        output = network(image)\n",
        "\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        predicted = torch.argmax(output, 1)\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == label).sum().item()\n",
        "\n",
        "        batch_losses[idx] = loss.item()\n",
        "\n",
        "    lesion_object_loss = np.mean(batch_losses)\n",
        "\n",
        "    o2o_normloss_change = (lesion_object_loss - base_mean_object_loss) / base_spread_object_loss\n",
        "\n",
        "    print(f'For Object Lesion: Normalized Object Loss Change: {o2o_normloss_change}')\n",
        "\n",
        "    network.features[28].weight.data = orig_weights\n",
        "    network.features[28].bias.data = orig_biases\n",
        "\n",
        "    return f2f_normloss_change, f2o_normloss_change, o2f_normloss_change, o2o_normloss_change\n"
      ],
      "metadata": {
        "id": "HJal1wSVv406"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can run this function as done in the cell below to perform the large lesion."
      ],
      "metadata": {
        "id": "blqDJUThEFAe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m-la1o7jqqu"
      },
      "outputs": [],
      "source": [
        "f2f, f2o, o2f, o2o = largelesiontest(dual_vgg, face_dataloader, object_dataloader,\n",
        "                                     face_ranking = face_ranking, object_ranking = object_ranking,\n",
        "                                     lesion_extent = 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3RMtEK7nX-f"
      },
      "source": [
        "Using these measures, we can define a measure of task segregation that is sensitive to how much a task-specific lesion changes the task it is specific to compared to the other task of the dual-task network. We will need to compute this index for each lesion. Then, we can take the mean value as an index for task segregation. The following is the formula for computing the task segregation index as devised by Katharina Dobs and colleagues (2022).\n",
        "\n",
        "\n",
        "$TS = \\frac{\\Delta L_{T1} - \\Delta L_{T2}}{\\Delta L_{T1} + \\Delta L_{T2}}$\n",
        "\n",
        "Where:\n",
        "\n",
        "$\\Delta L_{T1}$ is the normalized loss change of Task 1 (Faces)\n",
        "\n",
        "$\\Delta L_{T2}$ is the normalized loss change of the other Task (Objects)\n",
        "\n",
        "**ToDo (4)**:\n",
        "\n",
        "Compute the task segregation index for the 20% lesion. Then, compute the large lesion effects for a higher lesion extents, such as 50% and determine the task segregation index again. Keep in mind that you the rankings are only the Top50, so you will not be able to do a larger lesion than 50%. Also make sure to look at all lesion-specific segregation indexes in addition to the overall layer segregation index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72I8trE9n1Xi"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "...\n",
        "\n",
        "...\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can reveal the answer code by clicking \"Show code\". Use this if you no longer have computing credits, struggle or lack the time to run the code."
      ],
      "metadata": {
        "id": "VYzyffw0B-L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "f2f = 14.216\n",
        "f2o = 0.593\n",
        "o2f = 1.963\n",
        "o2o = 5.161\n",
        "\n",
        "face_seg_index_20 = (f2f - f2o) / (f2f + f2o)\n",
        "\n",
        "object_seg_index_20 = (o2o - o2f) / (o2o + o2f)\n",
        "\n",
        "seg_index_20 = (face_seg_index_20 + object_seg_index_20) / 2\n",
        "\n",
        "#f2f_50, f2o_50, o2f_50, o2o_50 = largelesiontest(dual_vgg, face_dataloader, object_dataloader,\n",
        "#                                     face_ranking = face_ranking, object_ranking = object_ranking,\n",
        "#                                     lesion_extent = 0.5)\n",
        "\n",
        "f2f_50 = 28.091\n",
        "f2o_50 = 5.546\n",
        "o2f_50 = 9.817\n",
        "o2o_50 = 10.814\n",
        "\n",
        "face_seg_index_50 = (f2f_50 - f2o_50) / (f2f_50 + f2o_50)\n",
        "\n",
        "object_seg_index_50 = (o2o_50 - o2f_50) / (o2o_50 + o2f_50)\n",
        "\n",
        "seg_index_50 = (face_seg_index_50 + object_seg_index_50) / 2\n",
        "\n",
        "print('')\n",
        "\n",
        "print(f'The Seg Index for the 20% Lesion is {seg_index_20}')\n",
        "print(f'Face Lesion Seg Index: {face_seg_index_20}')\n",
        "print(f'Object Lesion Seg Index. {object_seg_index_20}')\n",
        "print('')\n",
        "print(f'The Seg Index for the 50% Lesion is {seg_index_50}')\n",
        "print(f'Face Lesion Seg Index: {face_seg_index_50}')\n",
        "print(f'Object Lesion Seg Index. {object_seg_index_50}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p5YAQ3SsB-eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VrReNyxnYHX"
      },
      "source": [
        "**ToThink (3)**:\n",
        "\n",
        "How and why do the results change when we do a larger lesion, such as 50%? What does that suggest about the kernels in our network?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Write your answer here*"
      ],
      "metadata": {
        "id": "_itidm7MDkUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many other analyses and approaches you can take with lesioning, such as pruning to remove redundant kernels.\n",
        "\n",
        "**(OPTIONAL)**: Feel free to try this by lesioning the kernels with indices not included in the provided ranking!\n",
        "\n",
        "For an interesting paper on lesioning analysis have a look at this paper by [Bau and colleagues (2020)](ttps://doi.org/10.1073/pnas.1907375117)."
      ],
      "metadata": {
        "id": "D0PAVPQj-K-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concluding Remarks\n",
        "\n",
        "Congratulations for finishing this tutorial. I hope that you enjoyed this tutorial and learned some new methods that could be useful for you later! Here is a quick summary of what we just did and what you learned from it:\n",
        "\n",
        "\n",
        "\n",
        "*   We visually investigated a neural network by plotting feature maps to get a rough understanding of different tasks of different layers\n",
        "*   We lesioned a kernel to see its effect on the feature maps\n",
        "*   We used lesioning as a way to identify the task a kernel contributes to\n",
        "*   We optimized the input for a kernel using Gradient Ascent\n",
        "\n",
        "*   We used lesioning to measure how a network automatically segregates tasks\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5LlFdsEZESzR"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}